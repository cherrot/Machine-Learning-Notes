#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\begin_preamble
%中英文混排设置%
\usepackage[BoldFont,SlantFont,fallback,CJKchecksingle]{xeCJK}
\setmainfont{DejaVu Serif}%西文衬线字体
\setsansfont{DejaVu Sans}%西文无衬线字体
\setmonofont{DejaVu Sans Mono}%西文等宽字体
\setCJKmainfont{Adobe Song Std}%中文衬线字体
\setCJKsansfont{Adobe Heiti Std}%中文无衬线字体
\setCJKmonofont{WenQuanYi Micro Hei Mono}%中文等宽字体
\punctstyle{banjiao}%半角字符

%其他中文设置%
\XeTeXlinebreaklocale “zh”%中文断行
\XeTeXlinebreakskip = 0pt plus 1pt minus 0.1pt%左右弹性间距
\usepackage{indentfirst}%段落首行缩进
\setlength{\parindent}{2em}%缩进两个字符

%编号语言、样式设置%
\numberwithin{equation}{section}%设置公式按章节进行编号
\numberwithin{figure}{section}% 按章节编号
%\numberwithin{figure}{subsection}% 按子章节编号
\usepackage{footnpag}


%以下内容（\renewcommand）需要放置在\begin{document}之后才能起作用
\renewcommand\arraystretch{1.2}%1.2表示表格中行间距的缩放比例因子(缺省的标准值为1),中文需要更多的间距
\renewcommand{\contentsname}{目录} 
\renewcommand{\listfigurename}{插图目录} 
\renewcommand{\listtablename}{表格目录} 
\renewcommand{\refname}{参考文献} 
\renewcommand{\abstractname}{摘要} 
\renewcommand{\indexname}{索引} 
\renewcommand{\tablename}{表}
\renewcommand{\figurename}{图}
\renewcommand\appendixname{附录}
\renewcommand\partname{部分} 
\renewcommand\today{\number\year年\number\month月\number\day日}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding utf8-plain
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts true
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\font_cjk gbsn

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\float_placement !tbh
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered true
\pdf_bookmarksopen true
\pdf_bookmarksopenlevel 3
\pdf_breaklinks false
\pdf_pdfborder false
\pdf_colorlinks true
\pdf_backref section
\pdf_pdfusetitle false
\pdf_quoted_options "unicode=false"
\papersize a4paper
\use_geometry true
\use_amsmath 2
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 20mm
\topmargin 25mm
\rightmargin 20mm
\bottommargin 25mm
\footskip 15mm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\bullet 0 1 35 -1
\bullet 1 1 31 -1
\bullet 2 1 25 -1
\bullet 3 1 20 -1
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
renewcommand
\backslash
arraystretch{1.2}%1.2表示表格中行间距的缩放比例因子(缺省的标准值为1),中文需要更多的间距
\end_layout

\begin_layout Plain Layout


\backslash
renewcommand{
\backslash
contentsname}{目录} 
\end_layout

\begin_layout Plain Layout


\backslash
renewcommand{
\backslash
listfigurename}{插图目录} 
\end_layout

\begin_layout Plain Layout


\backslash
renewcommand{
\backslash
listtablename}{表格目录} 
\end_layout

\begin_layout Plain Layout


\backslash
renewcommand{
\backslash
refname}{参考文献} 
\end_layout

\begin_layout Plain Layout


\backslash
renewcommand{
\backslash
abstractname}{摘要} 
\end_layout

\begin_layout Plain Layout


\backslash
renewcommand{
\backslash
indexname}{索引} 
\end_layout

\begin_layout Plain Layout


\backslash
renewcommand{
\backslash
tablename}{表} 
\end_layout

\begin_layout Plain Layout


\backslash
renewcommand{
\backslash
figurename}{图} 
\end_layout

\begin_layout Plain Layout


\backslash
renewcommand
\backslash
appendixname{附录} 
\end_layout

\begin_layout Plain Layout


\backslash
renewcommand
\backslash
partname{部分} 
\end_layout

\begin_layout Plain Layout


\backslash
renewcommand
\backslash
today{
\backslash
number
\backslash
year年
\backslash
number
\backslash
month月
\backslash
number
\backslash
day日}
\end_layout

\end_inset


\end_layout

\begin_layout Title
Machine Learning Notes
\end_layout

\begin_layout Author
Cherrot Luo
\end_layout

\begin_layout Date
2011
\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Subsection
Supervised Learning
\end_layout

\begin_layout Standard
e.g.
 predict the price, predict cancer or not, etc.
\end_layout

\begin_layout Standard
We give the algorithm a data set in which the right answer was given.
\end_layout

\begin_layout Description
Regression
\begin_inset space ~
\end_inset

program Predict continuous valued output.
\end_layout

\begin_layout Description
Classification Discrete(离散的) valued output.
\end_layout

\begin_layout Subsection
Unsupervised Learning(Clustering)
\end_layout

\begin_layout Standard
e.g.
 Google News, Social network clustrering, 鸡尾酒派对问题 etc.
\end_layout

\begin_layout Description
singular
\begin_inset space ~
\end_inset

value
\begin_inset space ~
\end_inset

decomposition 奇异值分解。
\end_layout

\begin_layout Section
Linear Regression With One Variable
\end_layout

\begin_layout Subsection
Model Representation
\end_layout

\begin_layout Standard
卖房子问题
\end_layout

\begin_layout Description
Supervised
\begin_inset space ~
\end_inset

learning Given the 
\begin_inset Quotes eld
\end_inset

right answer
\begin_inset Quotes erd
\end_inset

 for each example in the data.
\end_layout

\begin_deeper
\begin_layout Description
Regression
\begin_inset space ~
\end_inset

Problem Predict real-valued output.(Classification Problem 是用来预测离散值的)
\end_layout

\end_deeper
\begin_layout Subsubsection
Training Set
\end_layout

\begin_layout Description
m Number of training examples.
\end_layout

\begin_layout Description
x'
\series medium
s
\series default
 
\begin_inset Quotes eld
\end_inset

input
\begin_inset Quotes erd
\end_inset

 variable / features.( often also called purchase)
\end_layout

\begin_layout Description
y'
\series medium
s 
\begin_inset Quotes eld
\end_inset

output
\begin_inset Quotes erd
\end_inset

 variable / 
\begin_inset Quotes eld
\end_inset

target
\begin_inset Quotes erd
\end_inset

 variable
\end_layout

\begin_layout Description
\begin_inset Formula $\left(x^{(i)},y^{(i)}\right)$
\end_inset

 通常的
\begin_inset Formula $\left(x_{i},y_{i}\right)$
\end_inset

。
\end_layout

\begin_layout Standard
单变量（Univariate）线性回归的直观认识可见图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:training-set"

\end_inset

。
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement tbh
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 2.1.training set.png
	scale 75

\end_inset


\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:training-set"

\end_inset

training set
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Cost Function
\end_layout

\begin_layout Standard
一元Hypothesis的公式
\begin_inset Formula $h_{\theta}\left(x\right)=\theta_{0}+\theta_{1}x$
\end_inset

,其实就是y=a*x+b。
\begin_inset Formula $\Theta_{i's}$
\end_inset

称为这个模型的参数(Parameters).问题要解决的就是如何选择这两个参数，使数据拟合的误差最小:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\underset{\theta_{0}\theta_{1}}{Minimize}\,\frac{1}{2m}\overset{m}{\underset{i=1}{\sum}}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)^{2}\label{eq: minimize cost function}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
定义cost
\begin_inset space ~
\end_inset

function为:
\begin_inset Formula 
\begin{equation}
J\left(\theta_{0},\theta_{1}\right)=\frac{1}{2m}\overset{m}{\underset{i=1}{\sum}}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)^{2}\label{eq:cost function of univariate hypothesis}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
(方差的二分之一)
\end_layout

\begin_layout Subsection
Cost function intuition I
\end_layout

\begin_layout Standard
如果
\begin_inset Formula $\theta_{0}=0$
\end_inset

,那么cost function 是二维坐标系的弓形（一元二次方程）。
\end_layout

\begin_layout Subsection
Cost function intuition II
\end_layout

\begin_layout Standard
cost function 如图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Cost-Function的直观表示"

\end_inset

所示，自变量是
\begin_inset Formula $\theta_{0}$
\end_inset

和
\begin_inset Formula $\theta_{1}$
\end_inset

(这种形状称为凸函数 convex funtion)：
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 2.3.cost function.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Cost-Function的直观表示"

\end_inset

Cost Function的直观表示
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
contour
\begin_inset space ~
\end_inset

plot（等值线图）可以更清楚直观的表示出Cost Function的收敛趋势，如图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:等值线图(Contour-Plot)"

\end_inset

所示。
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 2.3.contour plot.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:等值线图(Contour-Plot)"

\end_inset

等值线图(Contour Plot)
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Gradient descent -- 求最小J(
\begin_inset Formula $\theta_{0},\theta_{1}$
\end_inset

)的算法
\end_layout

\begin_layout Standard
\begin_inset Formula $\theta$
\end_inset

的初始值习惯上全部置为零（当然取别的值也无所谓）
\begin_inset Foot
status open

\begin_layout Plain Layout
在神经网络中，
\begin_inset Formula $\Theta$
\end_inset

的初始值却不能全部为0，详见
\begin_inset CommandInset ref
LatexCommand ref
reference "sub:Random-Initialization"

\end_inset

。
\end_layout

\end_inset

。使用该算法得到的是局部最优解，不一定是全局最优解，如图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Gradient-Descent的过程"

\end_inset

所示，如果起始点不同，那么可能会得到不同的局部最优解（local optimal）。
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 2.4.gradient descent.png
	lyxscale 80
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Gradient-Descent的过程"

\end_inset

Gradient Descent的过程
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
数学描述：
\end_layout

\begin_layout Standard
repeat until convergence{
\end_layout

\begin_layout Standard
\begin_inset space \qquad{}
\end_inset


\begin_inset Formula $\theta_{j}:=\theta_{j}-\alpha\frac{\partial}{\partial\theta_{j}}J\left(\theta_{0},\theta_{1}\right)\; for\, j=0\, and\, j=1$
\end_inset


\end_layout

\begin_layout Standard
}
\end_layout

\begin_layout Description
:= 赋值号
\end_layout

\begin_layout Description
\begin_inset Formula $\alpha$
\end_inset

 learning rate.
 Controls how big a step we take downhill with gradient descent.即梯度值，是个正数。
\end_layout

\begin_layout Description
偏导数(partial
\begin_inset space ~
\end_inset

derivative
\begin_inset space ~
\end_inset

term) 见下一节。偏导数和导数(用d而不是
\begin_inset Formula $\partial$
\end_inset

表示)的区别在于参数个数,该题中有两个参数，只对一个求导，所以是偏导(partial)。
\end_layout

\begin_layout Standard
注意，我们需要同时更新(Simultaneous update)
\begin_inset Formula $\theta_{0}$
\end_inset

和
\begin_inset Formula $\theta_{1}$
\end_inset

（见图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:同步更新"

\end_inset

）。
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 2.5. simultaneous update
	lyxscale 90
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:同步更新"

\end_inset

同步更新
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Gradient descent intuition
\end_layout

\begin_layout Standard
见图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:直观理解Gradient-Descent"

\end_inset

，以
\begin_inset Formula $\theta_{1}$
\end_inset

为例，如果落点在最小点右边，那么函数偏导是负数，
\begin_inset Formula $\theta_{1}$
\end_inset

的值会随之减小（向最小点靠近）；如果落点在最小点左边，那么求导结果是负数，那么
\begin_inset Formula $\theta_{1}$
\end_inset

的值会随之增大。最终的结果就是使
\begin_inset Formula $\theta_{1}$
\end_inset

落在或逼近最小点。
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 2.5.gradient descent intuition
	lyxscale 80
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:直观理解Gradient-Descent"

\end_inset

直观理解Gradient Descent
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Gradient descent for linear regression
\end_layout

\begin_layout Standard
“Batch
\begin_inset Quotes erd
\end_inset

 Gradient Descent.
 Each step of gradient descent uses all the training examples.
\end_layout

\begin_layout Section
Linear Algebra Review
\end_layout

\begin_layout Subsection
Matrices & Vectors
\end_layout

\begin_layout Standard
\begin_inset Formula $\mathbb{R}^{4\times2}$
\end_inset

表示4行2列的实数矩阵。
\end_layout

\begin_layout Description
Vector 
\begin_inset Formula $n\times1$
\end_inset

的矩阵。
\begin_inset Formula $\mathbb{R}^{n}$
\end_inset

表示n维的实数向量。
\end_layout

\begin_layout Subsection
加法
\end_layout

\begin_layout Subsection
矩阵X向量
\end_layout

\begin_layout Standard
矩阵X向量，矩阵的列数=向量的行数，得到的是另一个向量。
\end_layout

\begin_layout Standard
一般的矩阵乘法规则如下：
\end_layout

\begin_layout Standard
简单来说就是行X列，用“
\series bold
行列式
\series default
”来记忆它的运算顺序吧，恰好得到的结果是前面矩阵的行数X后面矩阵的列数的新矩阵。
\end_layout

\begin_layout Standard
做乘法必须保证前面矩阵的列数=后面矩阵的行数，可以用“
\series bold
前列腺
\series default
”来记忆……、
\end_layout

\begin_layout Subsection
矩阵X矩阵
\end_layout

\begin_layout Standard
很简单，矩阵和一个个的向量分别计算得到一个个的结果向量，然后组成新矩阵，可见图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:矩阵相乘示意图"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 3.4.matrix multiply.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:矩阵相乘示意图"

\end_inset

矩阵相乘示意图
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
矩阵乘法的性质
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 3.5.Not commutative.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:矩阵相乘不符合交换律"

\end_inset

矩阵相乘不符合交换律
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Enumerate
Not Commutative 不符合交换律，如图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:矩阵相乘不符合交换律"

\end_inset

所示。
\end_layout

\begin_layout Enumerate
associative符合结合律（也符合分配律）
\end_layout

\begin_layout Enumerate
Identity Matrix单位矩阵
\begin_inset Newline newline
\end_inset


\begin_inset Formula $A\cdot I=I\cdot A=A$
\end_inset

，这里I是单位矩阵，但注意两个I并不一定相同（
\begin_inset Formula $m\times n$
\end_inset

和
\begin_inset Formula $n\times m$
\end_inset

）。
\end_layout

\begin_layout Subsection
矩阵求逆Inverse和转置Transpose
\end_layout

\begin_layout Subsubsection
求逆：
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
AA^{-1}=A^{-1}A=I\label{eq:矩阵求逆}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
其中A为方阵（Square Matrix），只有方阵可以求逆。教材没有给出矩阵求逆的算法，而是用软件（Octave）实现的：pinv(A)
\end_layout

\begin_layout Subsubsection
转置
\end_layout

\begin_layout Standard
记矩阵
\begin_inset Formula $A$
\end_inset

的转置为
\begin_inset Formula $A^{T}$
\end_inset

，
\begin_inset Formula $A_{ij}=A_{ji}^{T}$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\left(AB\right)^{T}=B^{T}A^{T}\label{eq:转置矩阵的性质}
\end{equation}

\end_inset


\end_layout

\begin_layout Section
Linear Regression with Multiple Variables
\end_layout

\begin_layout Subsection
Multiple Features
\end_layout

\begin_layout Standard
Notation:
\end_layout

\begin_layout Standard
n = number of features
\end_layout

\begin_layout Standard
\begin_inset Formula $x^{\left(i\right)}$
\end_inset

 = input (features) of 
\begin_inset Formula $i^{th}$
\end_inset

 training example.
 对应训练集中的一条输入（一个向量）
\end_layout

\begin_layout Standard
\begin_inset Formula $x_{j}^{\left(i\right)}$
\end_inset

 = value of feature j in 
\begin_inset Formula $i^{th}$
\end_inset

 training example.
 向量
\begin_inset Formula $x^{\left(i\right)}$
\end_inset

中的第j项。
\end_layout

\begin_layout Standard
多个feature的Hypothesis：
\begin_inset Formula $h_{\theta}\left(x\right)=\theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}+\cdots+\theta_{n}x_{n}$
\end_inset


\end_layout

\begin_layout Standard
假设
\begin_inset Formula $x_{0}=1$
\end_inset

，且将x和
\begin_inset Formula $\theta$
\end_inset

均表示为向量，那么上式可写为：
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
h_{\theta}\left(x\right)=\theta^{T}x\label{eq:hypothesis-fuction(multiple-features)}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
通常被称作 multivariate linear regression（多元线性回归）
\end_layout

\begin_layout Subsection
Gradient descent for multiple variables
\end_layout

\begin_layout Standard
Hypothesis：
\begin_inset Formula $h_{\theta}\left(x\right)=\theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}+\cdots+\theta_{n}x_{n}$
\end_inset


\end_layout

\begin_layout Standard
Parameters: 将参数
\begin_inset Formula $\theta$
\end_inset

看作n+1维的向量
\begin_inset Formula $\theta$
\end_inset

。
\end_layout

\begin_layout Standard
Cost function: 
\begin_inset Formula $J\left(\theta\right)=\frac{1}{2m}\overset{m}{\underset{i=1}{\sum}}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)^{2}$
\end_inset


\end_layout

\begin_layout Standard
Gradient descent:
\end_layout

\begin_layout Standard
\begin_inset space \qquad{}
\end_inset

Repeat{
\end_layout

\begin_layout Standard
\begin_inset space \qquad{}
\end_inset


\begin_inset Formula $\theta_{j}:=\theta_{j}-\alpha\frac{\delta}{\delta\theta_{j}}J\left(\theta\right)$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset space \qquad{}
\end_inset

}
\end_layout

\begin_layout Standard
多变量Gradient Descent如图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:多变量的Gradient-Descent"

\end_inset

所示。
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 4.2.Gradient Descent.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:多变量的Gradient-Descent"

\end_inset

多变量的Gradient Descent
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Gradient descent in practice I:Feature Scaling
\begin_inset CommandInset label
LatexCommand label
name "sub:Gradient-descent-in-practice-1-feature-scaling"

\end_inset


\end_layout

\begin_layout Standard
Make sure features are on a similar scale。
\end_layout

\begin_layout Standard
推荐的方案是把Feature的值控制在-1到1（或其他一个相近的）范围内，便于快速收敛。
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement tbh
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 4.3.feature scaling.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Feature Scaling
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
如上例，两个feature相差3个数量级，这样就会导致 contour plot （等值线图）画出来非常的陡长（一个个很长的椭圆）。这样导致的后果便是，
 Gradient desent 需要很长的时间才能收敛到最优解。
\end_layout

\begin_layout Subsubsection
Mean（平均值） normalization
\end_layout

\begin_layout Standard
将
\begin_inset Formula $x_{i}$
\end_inset

替换为
\begin_inset Formula $x_{i}-\mu_{i}$
\end_inset

使features的平均值接近于0。
\end_layout

\begin_layout Standard
结合这两种方法，我们可以使
\begin_inset Formula $x_{i}$
\end_inset

取如图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Mean-Normalization"

\end_inset

所示的值。
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 4.3.Mean normalization.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Mean-Normalization"

\end_inset

Mean Normalization
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Gradient descent in practice II: Learning rate(
\series medium

\begin_inset Formula $\alpha$
\end_inset


\series default
)
\end_layout

\begin_layout Standard
\begin_inset Formula $\alpha$
\end_inset

太大可能不收敛，太小可能收敛太慢。来回试吧~
\end_layout

\begin_layout Subsection
Features and polynomial regression
\end_layout

\begin_layout Standard
可以根据已有的feature定义新的feature来简化模型。比如卖房子的例子，假定我们有房子的长和宽两个feature，那么我可以定义房子的面积作为一个新的f
eature，这样就只剩下一个feature就是房子的面积了。
\end_layout

\begin_layout Standard
另外可以使用多项式回归的方法，定义原feature的平方、立方等，增大数据拟合的精确度，如图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:多项式拟合"

\end_inset

所示。
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 4.5.Polynomial regression.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:多项式拟合"

\end_inset

多项式拟合
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Normal equation 正规方程
\begin_inset CommandInset label
LatexCommand label
name "sub:Normal-equation-正规方程"

\end_inset


\end_layout

\begin_layout Description
Normal
\begin_inset space ~
\end_inset

equation Method to solve for 
\begin_inset Formula $\theta$
\end_inset

 analytically.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 4.6.Normal equation.png
	width 70page%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
X为design matrix
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 4.6.Normal Equation3.png
	width 70page%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
构造design matrix
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Normal-equation"

\end_inset

构造Normal equation
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
将features和y表示成如图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Normal-equation"

\end_inset

所示的矩阵 (design matrix) 和向量的形式后，最小化cost function的
\begin_inset Formula $\theta$
\end_inset

值为（课程并没有给出这个公式的证明）：
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\theta=\left(X^{T}X\right)^{-1}X^{T}y\label{eq:normal equation}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
在Octave中表示为pinv( x' *x ) *x' *y。
\end_layout

\begin_layout Standard
使用normal equation时，feature scaling和mean normalization没有必要。
\end_layout

\begin_layout Standard
图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Gradient-Descent-Normal-Equation"

\end_inset

表示的是Gradient Descent和Normal Equation的区别：
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 4.6.Normal Equation2.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Gradient-Descent-Normal-Equation"

\end_inset

Gradient Descent 与 Normal Equation的区别
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
当n不是很大时，使用normal equation是个很好的选择。然而n很大时，normal equation的代价相当昂贵。
\end_layout

\begin_layout Subsection
Normal equation and non-invertibility(不可逆性)
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\theta=\left(X^{T}X\right)^{-1}X^{T}y
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $X^{T}X$
\end_inset

如果是不可逆的怎么办（虽然很少见）？（sigular/degenerate）
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
矩阵
\begin_inset Formula $A_{n\times n}$
\end_inset

可逆的充分必要条件是
\begin_inset Formula $A$
\end_inset

的行列式
\begin_inset Formula $\left|A\right|\neq0$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Octave中pinv和inv的区别：pinv是pseudo inverse,伪求逆，即使矩阵不可逆他也可以求得我们需要的结果。
\end_layout

\begin_layout Subsubsection
\begin_inset Formula $X^{T}X$
\end_inset

不可逆的原因？
\end_layout

\begin_layout Itemize
Redundant features(linearly dependenta存在
\series bold
线性
\series default
依赖).
\begin_inset Newline newline
\end_inset

E.g.
 
\begin_inset Formula $x_{1}=size\, in\, feet^{2}$
\end_inset

, 
\begin_inset Formula $x_{2}=size\, in\, m^{2}$
\end_inset


\end_layout

\begin_layout Itemize
Too many features(e.g.
 
\begin_inset Formula $m\leq n$
\end_inset

)
\begin_inset Newline newline
\end_inset

Delete some features, or use regularization(talk later).
\end_layout

\begin_layout Section
Octave Tutorial
\end_layout

\begin_layout Subsection
Basic operations
\end_layout

\begin_layout Itemize
声明矩阵：v=[1 2; 3 4]
\end_layout

\begin_layout Itemize
v= 1:0.2:2 的意思是，得到一个行向量（一行N列的矩阵），以1开始，到2结束，每次递增0.2。
\end_layout

\begin_layout Itemize
v=1:6 得到1到6递增1的整数行向量。
\end_layout

\begin_layout Itemize
disp()显示函数，使用它输出的结果不会带有ans=前缀。
\end_layout

\begin_layout Itemize
sprintf() 与C语言相似的格式输出函数。
\end_layout

\begin_layout Itemize
ones(2,3) 产生一个2*3的全是1的矩阵。
\end_layout

\begin_layout Itemize
zeros(3,5) 同理。
\end_layout

\begin_layout Itemize
rand(1,3)得到一个1*3的介于0到1的随机数矩阵。（如果是方阵，只写一个参数即可）
\end_layout

\begin_layout Itemize
randn(1,3) Gaussian random variable。得到的随机数呈0对称的高斯（正态）分布
\end_layout

\begin_layout Itemize
hist(w) 绘制w的分布直方图 hist(w,50)，显示50个统计条。
\end_layout

\begin_layout Itemize
eye(4) 产生4维的单位矩阵。eye是I的同音。
\end_layout

\begin_layout Subsection
Moving data around
\end_layout

\begin_layout Itemize
size(A)，返回A的尺寸，如果A是矩阵，则返回A的行数和列数。比如A是3*2的矩阵，那么该命令返回一个矩阵[3, 2]。如果执行size(A,
 1)将返回3，size(A, 2)返回2.
\end_layout

\begin_layout Itemize
length(A) 返回A较大的维数（可能是行数也可能是列数）。
\end_layout

\begin_layout Itemize
pwd 该变量保存Octave的当前工作目录（如/home/cherrot）。可以使用cd命令切换目录。
\end_layout

\begin_layout Itemize
who 用于显示当前环境中的所有变量。whos给出细节
\end_layout

\begin_layout Itemize
load 文件名 / load('文件名') ：加载数据文件。
\end_layout

\begin_layout Itemize
clear 变量名：清除变量。如果不跟变量则清除所有。
\end_layout

\begin_layout Itemize
v=priceY(1:10) ：将priceY的前10个元素赋值给v
\end_layout

\begin_layout Itemize
save hello.mat v ：将v存到文件hello.mat中。默认为二进制形式。可以加参数 -ascii 以存为文本格式。
\end_layout

\begin_layout Itemize
A(3,2) ：返回
\begin_inset Formula $A_{3,2}$
\end_inset

（从1开始, 1-indexed）
\begin_inset Newline newline
\end_inset

A(3,:) ：返回A第三行的所有元素。“:”代表该行/列的每一个元素
\begin_inset Newline newline
\end_inset

A( [1 3], :) ：返回第一行和第三行的所有元素
\end_layout

\begin_layout Itemize
A=[A, [5; 6; 7]] ：将向量[5; 6; 7]附加在A后面（右边）。
\end_layout

\begin_layout Itemize
A(:) 将A的所有元素打印成一个向量。
\end_layout

\begin_layout Itemize
C=[A B] ：将矩阵A、B左右拼接（也可以用逗号分隔）。
\end_layout

\begin_layout Itemize
C=[A; B] ：将矩阵A、B上下拼接。
\end_layout

\begin_layout Subsection
Computing on Data
\end_layout

\begin_layout Itemize
A*B 矩阵乘法
\end_layout

\begin_layout Itemize
Element wise operations 比如A .* B 将A中的
\series bold
每个元素
\series default
与B中对应
\series bold
元素
\series default
相乘构成新矩阵。
\begin_inset Newline newline
\end_inset

点号在常被用于指示element wise operations。比如A .^2 的结果是A中每个元素都平方。
\end_layout

\begin_layout Itemize
对矩阵应用log()或者abs()函数时，同样是element wise operation。对矩阵取反、加/减一个常数值、大小比较都将看作是element
 wise operation。
\end_layout

\begin_layout Itemize
A' ：A的转置。
\end_layout

\begin_layout Itemize
max(A)：如果A是矩阵，则返回A中每一列的最大元素；如果A是向量或行向量，则返回A中最大元素，而且如果写作[value, index] =
 max(A)，将返回值和其index。
\end_layout

\begin_layout Itemize
find(V<3)：返回向量V中小于3的元素下标
\begin_inset Newline newline
\end_inset

如果要find一个矩阵，可以写成[i, j] = find(A)。对应的i和j分别为找到元素的行列index。
\end_layout

\begin_layout Itemize
magic(n): 创建一个n*n (n!=2)的magic方阵。即每行、每列和两对角线上的元素和都为同一个数。
\end_layout

\begin_layout Itemize
sum() 求和。如果sum(A,
\series bold
1
\series default
)则对A每一列求和，等效于sum(A) 。
\begin_inset Newline newline
\end_inset

sum(A,
\series bold
2
\series default
)对每一行求和。
\end_layout

\begin_layout Itemize
prod() 求积
\end_layout

\begin_layout Itemize
floor()向下取整
\end_layout

\begin_layout Itemize
ceil()向上取整
\end_layout

\begin_layout Itemize
max(A, [], 
\series bold
1
\series default
) :column wise maximum。找到每列的最大值，得到一个行向量。1 means to take max along the first
 dimension of A.
\end_layout

\begin_layout Itemize
max(A, [], 
\series bold
2
\series default
) :row wise maximum。找到每行的最大值，得到一个向量
\end_layout

\begin_layout Itemize
要想找到矩阵A中的最大值，可以max(max(A)),或者max(A(:))——即先将A表示成一个向量，在求最大值。
\end_layout

\begin_layout Itemize
flipud() ：flip up & down，上下颠倒。故flipud(eye(3))的效果就是把单位矩阵的对角线对换。
\end_layout

\begin_layout Subsection
Plotting data
\end_layout

\begin_layout Standard
如果要在一个Figure面板中绘制两个曲线，那么需要调用hold on命令：
\begin_inset Newline newline
\end_inset

plot(t, y1);
\begin_inset Newline newline
\end_inset

hold on;
\begin_inset Newline newline
\end_inset

plot(t, y2, 'r'); % 'r'表示本事件的颜色为红色。
\begin_inset Newline newline
\end_inset

xlabel('time')
\begin_inset Newline newline
\end_inset

ylabel('value')
\begin_inset Newline newline
\end_inset

legend('sin', 'cos') %添加图例。
\begin_inset Newline newline
\end_inset

title('my plot')
\begin_inset Newline newline
\end_inset

print -dpng 'myPlot.png' %保存为png文件
\begin_inset Newline newline
\end_inset

close %关闭plot
\end_layout

\begin_layout Standard
如果要绘制多于一个的图像时：
\begin_inset Newline newline
\end_inset

figure(1); plot(t, y1);
\begin_inset Newline newline
\end_inset

figure(2); plot(t, y2);
\end_layout

\begin_layout Standard
如果要在一个figure中绘制两个图，可以使用subplot
\begin_inset Newline newline
\end_inset

subplot(1, 2, 1); %Divides plot a 1x2 grid, access first element
\begin_inset Newline newline
\end_inset

plot(t, y1);
\begin_inset Newline newline
\end_inset

subplot(1, 2, 2);
\begin_inset Newline newline
\end_inset

plot(t, y2);
\begin_inset Newline newline
\end_inset

axis([0.5 1 -1 1]); %改变右边图片的x y坐标范围，x从0.5到1，y从-1到1。
\begin_inset Newline newline
\end_inset

clf; %清空图像
\end_layout

\begin_layout Standard
如果要“绘制”一个矩阵：
\begin_inset Newline newline
\end_inset

A=magic(5) %生成一个矩阵
\begin_inset Newline newline
\end_inset

imagesc(A)
\end_layout

\begin_layout Standard
或者改成灰度显示一个矩阵：
\begin_inset Newline newline
\end_inset

imagesc(A), colorbar, colormap gray; %逗号可以使多个函数写在一行并且不屏蔽输出（异于分号）。
\end_layout

\begin_layout Subsection
For, While, If statements, and Functions
\end_layout

\begin_layout Subsubsection
For
\end_layout

\begin_layout Standard
e.g.1：
\end_layout

\begin_layout LyX-Code
v = zeros(10,1);
\end_layout

\begin_layout LyX-Code
for i=1:10,
\end_layout

\begin_deeper
\begin_layout LyX-Code
v(i) = 2^i;
\end_layout

\end_deeper
\begin_layout LyX-Code
end;
\end_layout

\begin_layout Standard
e.g.2：
\end_layout

\begin_layout LyX-Code
indices = 1:10; %indices 初始化为一个行向量
\end_layout

\begin_layout LyX-Code
for i = indices
\end_layout

\begin_deeper
\begin_layout LyX-Code
disp(i);
\end_layout

\end_deeper
\begin_layout LyX-Code
end;
\end_layout

\begin_layout Subsubsection
While
\end_layout

\begin_layout Standard
e.g.1
\end_layout

\begin_layout LyX-Code
i = 1;
\end_layout

\begin_layout LyX-Code
while i <= 5,
\end_layout

\begin_deeper
\begin_layout LyX-Code
v(i) = 100;
\end_layout

\begin_layout LyX-Code
i = i +1;
\end_layout

\end_deeper
\begin_layout LyX-Code
end;
\end_layout

\begin_layout Standard
e.g.2 使用break;
\end_layout

\begin_layout LyX-Code
i = 1;
\end_layout

\begin_layout LyX-Code
while true,
\end_layout

\begin_deeper
\begin_layout LyX-Code
v(i) = 999;
\end_layout

\begin_layout LyX-Code
i = i + 1;
\end_layout

\begin_layout LyX-Code
if i == 6,
\end_layout

\begin_deeper
\begin_layout LyX-Code
break;
\end_layout

\end_deeper
\begin_layout LyX-Code
end;
\end_layout

\end_deeper
\begin_layout LyX-Code
end;
\end_layout

\begin_layout Subsubsection
If-else
\end_layout

\begin_layout Standard
e.g.1
\end_layout

\begin_layout LyX-Code
if v(1) == 1,
\end_layout

\begin_deeper
\begin_layout LyX-Code
disp('one');
\end_layout

\end_deeper
\begin_layout LyX-Code
elseif v(1) == 2,
\end_layout

\begin_deeper
\begin_layout LyX-Code
disp('two');
\end_layout

\end_deeper
\begin_layout LyX-Code
else %这里没有逗号？
\end_layout

\begin_deeper
\begin_layout LyX-Code
disp('other');
\end_layout

\end_deeper
\begin_layout LyX-Code
end;
\end_layout

\begin_layout Subsubsection
Funtions
\end_layout

\begin_layout Standard
Octave定义函数的方式为把函数写四则算法在一个 function_name.m 文件中。文件内容范例：
\end_layout

\begin_layout LyX-Code
function y = function_name(x)
\end_layout

\begin_layout LyX-Code
y = x^2;
\end_layout

\begin_layout Standard
定义的函数文件必须在当前工作目录或者Octave的search path下。可以通过addpath('new_path')函数来增加search
 path
\end_layout

\begin_layout Standard
Octave的函数可以返回多值，比如返回[y1,y2]。
\end_layout

\begin_layout Subsection
Vectorization 向量化
\end_layout

\begin_layout Standard
把变量向量化使计算更加高效（Octave擅长矩阵运算），而且代码量更小（减少了循环等流程控制语句的使用）。
\end_layout

\begin_layout Section
Logistic Regression
\end_layout

\begin_layout Subsection
Classification
\end_layout

\begin_layout Standard
Classification 仍然属于 Supervised Learning，只不过是离散的值
\end_layout

\begin_layout Standard
应用场景：
\end_layout

\begin_layout Itemize
Email: Spam / Not Spam
\end_layout

\begin_layout Itemize
Online Transactions(交易): Fraudulent(欺诈)?
\end_layout

\begin_layout Itemize
Tumor(肿瘤); Malignant / Benign？
\end_layout

\begin_layout Standard
本节将讨论 binary classification， multi-class classification 将在以后讨论。
\end_layout

\begin_layout Standard
一种做法是继续使用线性回归
\begin_inset Formula $h_{\theta}\left(x\right)$
\end_inset

预测，然后定一个阈值(Threshold Classifier)，比如取0.5作为阈值，如果
\begin_inset Formula $h_{\theta}\left(x\right)\geq0.5$
\end_inset

，则预测y=1，否则预测y=0。但是这并不合适：
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 6.1 Binary claffication.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:使用线性回归处理Classification"

\end_inset

使用线性回归处理Classification的问题
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
正如图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:使用线性回归处理Classification"

\end_inset

所示，假设没有最右边的值，回归曲线应该是洋红色的这条，那么我们取阈值为0.5，预测的结果完全符合样本。然而，如果多了最右侧的这个取值时，回归曲线看起来应该像蓝色这
条一样，当我们取阈值0.5时可以发现，预测值不再符合样本了。
\end_layout

\begin_layout Description
Logistic
\begin_inset space ~
\end_inset

Regression: 总能产生
\begin_inset Formula $0\leq h_{\theta}\leq1$
\end_inset

的
\begin_inset Formula $h_{\theta}$
\end_inset

的算法（在y=0 或 y=1 的binary classification的情况下）。
\end_layout

\begin_layout Subsection
Hypothesis Representation
\end_layout

\begin_layout Standard
为了得到
\begin_inset Formula $0\leq h_{\theta}\left(x\right)\leq1$
\end_inset

，则定义
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
h_{\theta}\left(x\right)=g\left(\theta^{T}x\right)\label{eq:logistic regression hypothesis}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
其中
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
g\left(z\right)=\frac{1}{1+e^{-z}}\label{eq:sigmoid function}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
上式称为sigmoid function（S形函数) 或 logistic function（逻辑函数）。
\begin_inset Formula $\theta^{T}x$
\end_inset

可见公式
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:hypothesis-fuction(multiple-features)"

\end_inset

。公式
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:sigmoid function"

\end_inset

的图形如所示：
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 6.2.sigmoid function.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
sigmoid function
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Interpretation(解释) of Hypothesis Output
\end_layout

\begin_layout Standard
\begin_inset Formula $h_{\theta}\left(x\right)=P\left(y=1\mid x;\,\theta\right)$
\end_inset

.
 It 
\series bold
estimats
\series default
 probability that y=1, given x, parameterized by 
\begin_inset Formula $\theta$
\end_inset

.
 
\end_layout

\begin_layout Subsection
Decision Boundary
\end_layout

\begin_layout Standard
只有当
\begin_inset Formula $z\geq0$
\end_inset

时，
\begin_inset Formula $g\left(z\right)\geq0.5$
\end_inset

才成立，故只有
\begin_inset Formula $\theta^{T}x\geq0$
\end_inset

时，
\begin_inset Formula $h_{\theta}\left(x\right)\geq0.5$
\end_inset

才成立。
\end_layout

\begin_layout Standard
Decision Boundary 就是使
\begin_inset Formula $\theta^{T}x=0$
\end_inset

的直线，如图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Decision-Boundary"

\end_inset

所示：
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 6.3.Decision boundary.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Decision-Boundary"

\end_inset

Decision Boundary
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Decision-Boundary"

\end_inset

中
\begin_inset Formula $x_{1}+x_{2}=3$
\end_inset

这条线就是Decision Boundary。
\end_layout

\begin_layout Subsection
Cost function
\end_layout

\begin_layout Standard
如果将线性回归中的 cost function 用到这里的话，那么我们得到的将是一个 non-convex （非凸函数）的图形（因为
\begin_inset Formula $h_{\theta}\left(x\right)$
\end_inset

不是一个线性函数，而是一个平方代价函数squre cost function），导致得不到全局最优解，如图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:直接使用线性回归的cost-function导致结果为非凸函数"

\end_inset

所示。
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 6.4.Cost Function
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:直接使用线性回归的cost-function导致结果为非凸函数"

\end_inset

直接使用线性回归的cost function导致结果为非凸函数
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
下面是Classification情况下的Cost function(with single training example)：
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
J\left(\theta\right)=\frac{1}{m}\sum_{i=1}^{m}Cost\left(h_{\theta}\left(x^{\left(i\right)}\right),y^{\left(i\right)}\right)\label{eq:cost function in classification}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
其中,
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
Cost\left(h_{\theta}\left(x\right),y\right)=\begin{cases}
-\log\left(h_{\theta}\left(x\right)\right) & if\, y=1\\
-\log\left(1-h_{\theta}\left(x\right)\right) & if\, y=0
\end{cases}\label{eq:Cost function in classifcation-Cost-2line}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
函数的图形可以直观的告诉我们上式的作用：
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 6.3.Plot Cost function.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Plotting the Cost()
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
为方便计算，公式
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Cost function in classifcation-Cost-2line"

\end_inset

可写为式
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Cost function in classifcation-Cost-1line"

\end_inset

的形式。
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
Cost\left(h_{\theta}\left(x\right),y\right)=-y\log\left(h_{\theta}\left(x\right)\right)-\left(1-y\right)\log\left(1-h_{\theta}\left(x\right)\right)\label{eq:Cost function in classifcation-Cost-1line}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
这里要注意当y=1时它的几个特性，如图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Cost函数(Classification)"

\end_inset

所示的。在y=1时，当
\begin_inset Formula $h_{\theta}\left(x\right)=1$
\end_inset

时，Cost为0，也就是没有误差；相反，若
\begin_inset Formula $h_{\theta}\left(x\right)\rightarrow0$
\end_inset

，那么则视为误差无限大，即Cost趋向于无穷。
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 6.3.Cost function.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Formula $Cost\left(h_{\theta}\left(x^{\left(i\right)}\right),y^{\left(i\right)}\right)$
\end_inset

函数
\begin_inset CommandInset label
LatexCommand label
name "fig:Cost函数(Classification)"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
The topic of convexity analysis is beyond the scope of this course.
\end_layout

\begin_layout Subsection
Simplified cost function and gradient descent
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
J\left(\theta\right)=\frac{1}{m}\sum_{i=1}^{m}Cost\left(h_{\theta}\left(x^{\left(i\right)}\right),y^{\left(i\right)}\right)=-\frac{1}{m}\left[\sum_{i=1}^{m}y^{\left(i\right)}\log h_{\theta}\left(x^{\left(i\right)}\right)+\left(1-y^{\left(i\right)}\right)\log\left(1-h_{\theta}\left(x^{\left(i\right)}\right)\right)\right]\label{eq:simplified cost function for logistic regression}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
之所以使用这个公式作为Cost Function而不是使用其他的公式是因为 This cost function can be derived
 from statistics using the 
\series bold
principle of maximum likelihood estimation（最大似然估计）,
\series default
 and it is convex.
 更多的细节超出了本课程的讨论范围。
\end_layout

\begin_layout Standard
给出上式，我们需要寻找到合适的
\begin_inset Formula $\theta$
\end_inset

满足
\begin_inset Formula $\underset{\theta}{\min}J\left(\theta\right)$
\end_inset

，从而就可以计算
\begin_inset Formula $h_{\theta}\left(x\right)$
\end_inset

了。
\end_layout

\begin_layout Subsubsection
Gradient Descent
\end_layout

\begin_layout Standard
Repeat{
\end_layout

\begin_layout Standard
\begin_inset Formula $\theta_{j}\,:=\theta_{j}-\alpha\frac{\delta}{\delta\theta_{j}}J\left(\theta\right)$
\end_inset


\end_layout

\begin_layout Standard
}(simultaneously update all 
\begin_inset Formula $\theta_{j}$
\end_inset

)
\end_layout

\begin_layout Standard
其中
\begin_inset Formula $\frac{\delta}{\delta\theta_{j}}J\left(\theta\right)=\frac{1}{m}\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)x_{j}^{\left(i\right)}$
\end_inset

，和线性回归的结果
\series bold
相似
\begin_inset Foot
status open

\begin_layout Plain Layout

\series bold
怎么算的？
\series default
 我的猜想是，sigmoid函数只是起函数域的限定作用，不改变原函数的增长性，所以为了计算简便，求导时不考虑sigmoid函数。求证。
\end_layout

\end_inset


\series default
，见
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:多变量的Gradient-Descent"

\end_inset

。
\series bold

\begin_inset CommandInset label
LatexCommand label
name "why:gradient descent求导公式疑问"

\end_inset


\end_layout

\begin_layout Standard
该算法同样可以使用feature scaling（见
\begin_inset CommandInset ref
LatexCommand ref
reference "sub:Gradient-descent-in-practice-1-feature-scaling"

\end_inset

节）来优化计算。
\end_layout

\begin_layout Subsection
Advanced Optimization
\end_layout

\begin_layout Standard
Optimaization algorithms:
\end_layout

\begin_layout Itemize
Gradient descent
\end_layout

\begin_layout Itemize
Conjugate gradient
\end_layout

\begin_layout Itemize
BFGS
\end_layout

\begin_layout Itemize
L-BFGS
\end_layout

\begin_layout Standard
其他三个算法具体是怎么做的将不在本课程中涉及。这些算法的缺点是非常复杂，而优点是：
\end_layout

\begin_layout Itemize
No need to manually pick 
\begin_inset Formula $\alpha$
\end_inset


\end_layout

\begin_layout Itemize
Often faster than gradient descent.
\end_layout

\begin_layout Standard
We can think of these algorithms as having a clever inter-loop.
 In fact the inter-loop is called the 
\series bold
line search
\series default
 algorithm that automatically tries out different values for 
\begin_inset Formula $\alpha$
\end_inset

 and automatically picks a good learning rate so that it can even pick a
 different alpha for every interation.
\end_layout

\begin_layout Standard
本课程将不详细的讨论这些算法内部究竟是怎样工作的。作者使用这些算法好多年后才去探索它们是如何工作的=.=
\end_layout

\begin_layout Standard
而且不建议自己写代码实现这些代码，有类库干吗不用？例如使用octave的自建函数fminunc更好的实现gradient descent：（图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:fminunc函数的使用示例"

\end_inset

）
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 6.6.Gradient Example using Octave.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:fminunc函数的使用示例"

\end_inset

fminunc函数的使用示例
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
fminunc()函数：function minimization unconstrained。 是Octave中的内建函数，@表示一个指向函数的指针
 (function handle)。fminunc的作用就是计算使给定函数取值最小的参数值，如图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Octave演示fminunc函数"

\end_inset

所示即为Octave的演示：
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 6.6.Exercise using Octave.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Octave演示fminunc函数"

\end_inset

Octave演示fminunc函数
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
options是存储我们需要的option的数据结构。 'GradObj', 'on' sets the gradient objective
 parameter to on.
 It just means you are indeed going to provide a gradient to this algorithm.
 其中exitFlag=1表示算法成功收敛(converge)了。optTheta和functionVal都得到了期望的值（functionVal=0）。注意要
使用这个函数，initialTheta必须是不小于二维的向量。
\end_layout

\begin_layout Itemize
Octvave语法：@(t) ( costFunction(t, X, y) ) .
 This creates a function, with argument t, which calls your costFunction.（实例见编程作业
2）
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 6.6.Cost Function in Octave.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:使用fminunc时需要自定义的函数"

\end_inset

使用fminunc时需要自定义的函数
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
What we need to do is 
\series bold
write a function that returns the cost and returns the gradient
\series default
（见图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:使用fminunc时需要自定义的函数"

\end_inset

）.
 当然，我们甚至可以将这个算法使用在线性回归问题上。
\end_layout

\begin_layout Subsection
Multi-class classification: One-vs-All
\end_layout

\begin_layout Standard
实例：
\end_layout

\begin_layout Itemize
标记E-mail类型：Wor, Friends, Family, Hobby。
\end_layout

\begin_layout Itemize
诊断：Not ill, Cold, Flu
\end_layout

\begin_layout Itemize
天气：Sunny, Cloudy, Rain, Snow
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 6.7.Multi class classification:One-vs-All
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Multi-class-classification-问题"

\end_inset

Multi-class classification 问题
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
One-vs-all(one-vs-rest)
\end_layout

\begin_layout Standard
\begin_inset Wrap figure
lines 0
placement o
overhang 0in
width "0col%"
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename 6.7.multi-classification
	scale 75

\end_inset


\end_layout

\end_inset

What we are going to do is take a training set, and turn this into three
 
\series bold
separate binary classification problems:
\series default
(如图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Multi-class的分解"

\end_inset

所示)
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement tb
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename 6.7.ones-vs-all1
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename 6.7.ones-vs-all2.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename 6.7.ones-vs-all3.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Multi-class的分解"

\end_inset

Multi-class的分解
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
h_{\theta}\left(x\right)=P\left(y=i\mid x;\theta\right),\,\,\left(i=1,2,3\right)\label{eq:hypothesis of multi-class:one-vs-all}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Train a logistic regression classifier 
\begin_inset Formula $h_{\theta}^{\left(i\right)}\left(x\right)$
\end_inset

 for each class 
\begin_inset Formula $i$
\end_inset

 to predict the probability that 
\begin_inset Formula $y=i$
\end_inset

.
\end_layout

\begin_layout Standard
On a new input 
\begin_inset Formula $x$
\end_inset

, to make a prediction, pick the class 
\begin_inset Formula $i$
\end_inset

 that maximizes 
\begin_inset Formula $\max_{i}h_{\theta}^{\left(i\right)}\left(x\right)$
\end_inset

.
\end_layout

\begin_layout Section
Regularization
\end_layout

\begin_layout Subsection
The problem of overfitting
\end_layout

\begin_layout Standard
features 太少导致 underfitting(high bias)，而features太多就会导致overfitting(high variants)，
正如图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Underfitting-and-Overfitting-in-linear-regression"

\end_inset

和图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Underfitting-and-Overfitting-in-logistic"

\end_inset

所示。
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 7.1.overfitting.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Underfitting-and-Overfitting-in-linear-regression"

\end_inset

Underfitting and Overfitting in Linear Regression
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Description
Overfitting If we have too many features, the learned hypothesis may fit
 the training set very well(
\begin_inset Formula $J\left(\theta\right)=\frac{1}{2m}\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)^{2}\approx0$
\end_inset

), but fail to generalize to new examples(predict prices on new examples).
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 7.1.overfitting2.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Underfitting-and-Overfitting-in-logistic"

\end_inset

Underfitting and Overfitting in Logistic Regression
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Addressing overfitting
\end_layout

\begin_layout Standard
Options:
\end_layout

\begin_layout Enumerate
Reduce number of features.
\end_layout

\begin_deeper
\begin_layout Itemize
Manually select which features to keep.
\end_layout

\begin_layout Itemize
Model selection algorithm.
\end_layout

\end_deeper
\begin_layout Enumerate
Regularization
\end_layout

\begin_deeper
\begin_layout Itemize
Keep all the features, but reduce magnitude/values of parameters 
\begin_inset Formula $\theta_{j}$
\end_inset

.（减小参数的权重）
\end_layout

\begin_layout Itemize
Works well when we have a lot of features, each of which contributes a bit
 to predicting 
\begin_inset Formula $y$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Subsection
Cost function
\end_layout

\begin_layout Subsubsection
Intuition
\end_layout

\begin_layout Standard
如图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Penalize-parameters"

\end_inset

所示，如果出现了过度拟合，我们可以使
\begin_inset Formula $\theta_{3}$
\end_inset

和
\begin_inset Formula $\theta_{4}$
\end_inset

变得非常小甚至接近于0，从而降低这两个参数对整个图形的贡献。如图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Penalize-parameters"

\end_inset

所示，在原式后面加上
\begin_inset Formula $1000\theta_{3}^{2}+1000\theta_{4}^{2}$
\end_inset

两个因子（其中1000是随便指定的比较大的值），这样，如果要使原式取最小值，那我们必须使
\begin_inset Formula $\theta_{3}$
\end_inset

和
\begin_inset Formula $\theta_{4}$
\end_inset

取值很小，从而达到我们的目的。
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 7.2.Intuition.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Penalize-parameters"

\end_inset

Penalize parameters instead of removing them
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Regularization
\end_layout

\begin_layout Standard
Small values for parameters 
\begin_inset Formula $\theta_{0},\theta_{1},\ldots,\theta_{n}$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Quotes eld
\end_inset

Simpler
\begin_inset Quotes erd
\end_inset

 hypothesis
\end_layout

\begin_layout Itemize
Less prone(倾向) to overfitting.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 7.2.Intuition2.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Regularization"

\end_inset

Regularization
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
如图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Regularization"

\end_inset

所示
\begin_inset Foot
status open

\begin_layout Plain Layout
注意j是从1开始取值的。这是个习惯上的做法，实际上从0开始或从1开始对结果不会有太大影响(little difference)。
\end_layout

\end_inset

，Regularization对
\series bold
每个
\series default
参数(
\begin_inset Formula $\theta$
\end_inset

)都乘以一个惩罚因子
\begin_inset Formula $\lambda$
\end_inset

。What the regularization parameter does is it controls the tradeoff between
 the goal of fitting the data well and the goal of keeping the parameter
 small, and therefore keeping the hypothesis simple.
\end_layout

\begin_layout Standard
但是如果
\begin_inset Formula $\lambda$
\end_inset

取值太大，则会造成underfitting。道理很简单，right?
\end_layout

\begin_layout Subsection
Regularized linear regression
\end_layout

\begin_layout Subsubsection
Gradient descent
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 7.3.Regularized linear regression.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Gradient-Descent-with-Regularization"

\end_inset

Gradient Descent with Regularization
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
注意 
\begin_inset Formula $1-\alpha\frac{\lambda}{m}$
\end_inset

 是小于1的，这就相当于把 
\begin_inset Formula $\theta_{j}$
\end_inset

缩小了，而后面的部分和之前的公式是一样的。中间公式中括号内的部分正是对 
\begin_inset Formula $J\left(\theta\right)$
\end_inset

 求导的结果。 注意，这里有一处符号错误，
\begin_inset Formula $\frac{\lambda}{m}\theta_{j}$
\end_inset

 前应该是+。
\end_layout

\begin_layout Subsubsection
Normal equation
\end_layout

\begin_layout Standard
Gradient descent 只是两种线性回归拟合算法中的一个，另外一种是基于 normal equation 的，我们需要一个 design
 matrix 
\begin_inset Formula $X_{m\times\left(n+1\right)}$
\end_inset

和结果矢量y。X的每一行对应一个 training example。y保存着每个training example的结果。关于算法的详细描述可参见
\begin_inset CommandInset ref
LatexCommand ref
reference "sub:Normal-equation-正规方程"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\theta=\left(X^{T}X+\lambda\begin{bmatrix}0\\
 & 1\\
 &  & \ddots\\
 &  &  & 1
\end{bmatrix}\right)^{-1}X^{T}y\label{eq:Normal-equation-with-regulariazion}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
上式中
\begin_inset Formula $\lambda$
\end_inset

后面的矩阵是 
\begin_inset Formula $\left(n+1\right)\times\left(n+1\right)$
\end_inset

的，如果去掉
\begin_inset Formula $\lambda$
\end_inset

和其后的矩阵，那么就是之前学过的的Normal Equation，这里是加上Regularization。其实等号后的部分正是令 cost function
 求导为0的结果：
\begin_inset Formula $\frac{\delta}{\delta\theta_{j}}J\left(\theta\right)\overset{set}{=}0$
\end_inset

。由于证明比较复杂，因此具体为什么是这个结果老师不作解释:D
\end_layout

\begin_layout Standard
我们知道，当 
\begin_inset Formula $m\leq n$
\end_inset

 时，公式
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:normal equation"

\end_inset

中的
\begin_inset Formula $X^{T}X$
\end_inset

 是不可逆的，然而可以证明，当使用 Regularization后，加上Regularization因子后的矩阵一定是可逆的。
\end_layout

\begin_layout Subsection
Regularized logistic regression
\end_layout

\begin_layout Subsubsection
Gradient Descent
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 7.4.Regularized logistic regression.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Cost-function-of-logistic-with-reg"

\end_inset

Cost function of logistic with reguarization
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
J\left(\theta\right)=-\left[\frac{1}{m}\sum_{i=1}^{m}\left(y^{\left(i\right)}\log h_{\theta}\left(x^{\left(i\right)}\right)+\left(1-y^{\left(i\right)}\right)\log\left(1-h_{\theta}\left(x^{\left(i\right)}\right)\right)\right)\right]+\frac{\lambda}{2m}\sum_{j=1}^{n}\theta_{j}^{2}\label{eq:cost func logistic regression with reg}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
在 logistic regression 中使用Regularization的cost function见图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Cost-function-of-logistic-with-reg"

\end_inset

和公式
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:cost func logistic regression with reg"

\end_inset

，需要注意j仍然是从1开始的。
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 7.4.CostFuntion.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Gradient-Descent-of-logistic-with-reg"

\end_inset

Gradient Descent of Logistic Regression with Regularization
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
经过Regularized的gradient descent如图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Gradient-Descent-of-logistic-with-reg"

\end_inset

所示，该图中仍有一处符号错误：
\begin_inset Formula $\frac{\lambda}{m}\theta_{j}$
\end_inset

 前应该是+。注意，虽然看起来图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Gradient-Descent-of-logistic-with-reg"

\end_inset

中的公式和线性回归中的类似，但该图中的
\begin_inset Formula $h_{\theta}$
\end_inset

和线性回归中的
\begin_inset Formula $h_{\theta}$
\end_inset

是完全不同的函数。
\end_layout

\begin_layout Standard
同上一节一样，中括号中的部分是 
\begin_inset Formula $J\left(\theta\right)$
\end_inset

 的偏导。
\end_layout

\begin_layout Subsubsection
Advanced optimization
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 7.4.Advanced optimization.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Advanced-Optimization"

\end_inset

Advanced Optimization
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
要想使用regularized的advanced optimization，只需要将 cost function 的定义进行如图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Advanced-Optimization"

\end_inset

所示的修改即可，注意图中仍然存在符号错误。
\end_layout

\begin_layout Section
Neural Networks: Representation
\end_layout

\begin_layout Subsection
Non-linear Hypotheses
\end_layout

\begin_layout Standard
当输入的feature集巨大时，Logistic Regression将会力不从心。神经网络是用于计算复杂非线性回归问题(complex non-linear
 hypotheses)的好方法。
\end_layout

\begin_layout Subsection
Neurons（神经元） and the Brain
\end_layout

\begin_layout Description
Neuro-rewiring
\begin_inset space ~
\end_inset

experiments 比如把传入听觉中枢的神经re-wire到眼睛，那么听觉中枢会学习用眼睛看的功能。同一块大脑组织，其实可以处理听觉、视觉、触觉等等功能。看
起来大脑有非常强大的学习算法:)
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 8.2.Neurons and brain.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
大脑强大的学习能力
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Model representation I
\end_layout

\begin_layout Subsubsection
Nuron in the brain
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 8.3.1.nuron in the brain
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:大脑中的神经元结构"

\end_inset

大脑中的神经元结构
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
大脑中的神经元结构如图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:大脑中的神经元结构"

\end_inset

所示，图中 Dendrite 是树突，也就是输入端，Axon是轴突，也就是输出端。
\end_layout

\begin_layout Subsubsection
Neuron model: losgistic unit
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 8.3.1.neuron model:losistic unit.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:人工神经元模型：逻辑单元"

\end_inset

人工神经元模型：逻辑单元
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
我们将大脑中的神经元类比到人工智能网络中，一个简单的神经网络可见图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:人工神经元模型：逻辑单元"

\end_inset

。
\end_layout

\begin_layout Standard
一般绘制神经网络图时会省略
\begin_inset Formula $x_{0}$
\end_inset

，
\begin_inset Formula $x_{0}=1$
\end_inset

 称为bias unit或bias neuron。
\end_layout

\begin_layout Standard
When we talk about neural networks, sometimes we'll say that this is a neural
 network with a sigmoid(logistic) 
\series bold
activation function
\series default
(激活函数).
 This activation function in the neural network terminology is just another
 term for that non-linear function 
\begin_inset Formula $g\left(z\right)=\frac{1}{1+e^{-z}}$
\end_inset

.
\end_layout

\begin_layout Standard
回归模型中的参数（
\begin_inset Formula $\theta$
\end_inset

）在神经网络中有时被称为权重（weights）。
\end_layout

\begin_layout Subsubsection
Neural Network
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 8.3.Neural Network.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Neural-Network"

\end_inset

Neural Network
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Neural-Network"

\end_inset

是一个三层的神经网络，其中
\begin_inset Formula $x_{1}$
\end_inset

,
\begin_inset Formula $x_{2}$
\end_inset

,
\begin_inset Formula $x_{3}$
\end_inset

是输入层，The third layer outputs the value that the hypothesis 
\begin_inset Formula $h\left(x\right)$
\end_inset

 computes.第一层经常称为输入层(input layer)，最后一层被称为输出层(output layer)，而位于中间的被称为隐藏层(hidden
 layer)。关于这个神经网络是如何工作的，请参考图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Neural-Network(2)"

\end_inset

。
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 8.3.Neural Network2.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Neural-Network(2)"

\end_inset

Neural Network(2)
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
需要注意图中的标号，正如图中所说，如果神经网络在第
\begin_inset Formula $j$
\end_inset

层有
\begin_inset Formula $s_{j}$
\end_inset

个单位，在
\begin_inset Formula $j+1$
\end_inset

层有
\begin_inset Formula $s_{j+1}$
\end_inset

个单位，那么
\begin_inset Formula $\Theta^{\left(j\right)}$
\end_inset

将是一个
\begin_inset Formula $s_{j+1}\times\left(s_{j}+1\right)$
\end_inset

的矩阵，如h图中
\begin_inset Formula $\Theta^{\left(1\right)}$
\end_inset

就是
\begin_inset Formula $3\times4$
\end_inset

的矩阵。
\end_layout

\begin_layout Subsection
Model representation II
\end_layout

\begin_layout Subsubsection
Forward propagation: Vectorized implementation 前向传播：矢量化实现
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 8.4.forward propagation.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Forward-propagation:Vectorized-impl"

\end_inset

Forward propagation:Vectorized implementation
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
如图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Forward-propagation:Vectorized-impl"

\end_inset

所示，将括号中的部分表示为
\begin_inset Formula $z_{i}^{\left(j\right)}$
\end_inset

，那么计算过程可以使用图中右半部分的矩阵运算表示（输入层
\begin_inset Formula $x$
\end_inset

使用
\begin_inset Formula $a^{\left(1\right)}$
\end_inset

表示）。We call this process of computing 
\begin_inset Formula $h_{\theta}\left(x\right)$
\end_inset

 
\series bold
forward propagation
\series default
 because that we start of with the 
\series bold
activations
\series default
 of the input-units and then we 
\series bold
forward propagate
\series default
 that to the hidden layer and compute the activations of the hidden layer
 and then we forward propagate that, and compute the activations of the
 output layer, this process of computing the activations from the input
 then the hidden then the output layer and that's also called forward propagatio
n.
\end_layout

\begin_layout Subsubsection
Neural Network learning its own features
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 8.4.Neural network learning features.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Neural-network-learning-its-own-features"

\end_inset

Neural network learning its own features
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
如图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Neural-network-learning-its-own-features"

\end_inset

所示，如果将 input layer 遮住，那么计算 
\begin_inset Formula $h_{\theta}\left(x\right)$
\end_inset

 的公式将如图中所示，可见与 Logistic Regression 的公式（公式
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:logistic regression hypothesis"

\end_inset

）几乎相同（除了
\begin_inset Formula $\theta$
\end_inset

变成了大写
\begin_inset Formula $\Theta$
\end_inset

外~）。只不过在神经网络中，我们并不直接使用输入的 features -- 
\begin_inset Formula $x_{1},x_{2},x_{3}$
\end_inset

，而是使用 hidden layer 的 activations -- 
\begin_inset Formula $a_{1}^{\left(2\right)},a_{2}^{\left(2\right)},a_{3}^{\left(2\right)}$
\end_inset

。 It learned its own features to apply logistic regression.
 这将比使用原始的输入 features （或他们的多项式形式 
\begin_inset Formula $x_{1}^{2},\, x_{1}x_{2},\, x_{2}^{2}\ldots$
\end_inset

）来计算 logistic regression 产生更好的 hypotheses。
\end_layout

\begin_layout Subsection
Examples and intuitions I
\end_layout

\begin_layout Subsubsection
Simple example: AND
\end_layout

\begin_layout Standard
图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:神经网络中的AND逻辑"

\end_inset

为神经网络实现的AND逻辑。
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 8.5.Example1.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:神经网络中的AND逻辑"

\end_inset

神经网络中的AND逻辑
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Examples and intuitions II
\end_layout

\begin_layout Standard
在本节将进行更为复杂的非线性问题的求解。
\end_layout

\begin_layout Standard
图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:XNOR逻辑的实现过程"

\end_inset

向我们展示了XNOR逻辑的具体实现过程。XNOR的真值表见图右下部分，即“相同为真，相异为假”，若以
\begin_inset Formula $x_{1}$
\end_inset

和
\begin_inset Formula $x_{2}$
\end_inset

为坐标轴，那么我们需要确定图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:XNOR逻辑的实现过程"

\end_inset

上部所示坐标系的 decision boundary。通过组合图中上部的三个简单神经网络，可以构造出XNOR的神经网络，即中间一层使用AND和
 NOT AND，输出层使用OR，得到的结果可以使用真值表检验。
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 8.6.examples2.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:XNOR逻辑的实现过程"

\end_inset

XNOR逻辑的实现过程
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 8.6.examples3.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:神经网络的经典案例：数字识别"

\end_inset

神经网络的经典案例：数字识别
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Multi-class classification
\end_layout

\begin_layout Subsubsection
Multiple output units: One-vs-all
\end_layout

\begin_layout Standard
如图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:One-vs-All-in-neural-net"

\end_inset

，四个输出项分别标识了是否是步行、是否是轿车、是否时摩托车和是否是卡车。可见类似于之前的One-vs-All方法，这里有四个逻辑标识符(logistic
 classifier)。注意图中用的是约等号。
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 8.7.1.One vs All.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:One-vs-All-in-neural-net"

\end_inset

One vs All in Neural Network
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
课后习题：Suppose you have a multi-class classification problem with three classes,
 trained with a 3 layer network.
 Let 
\begin_inset Formula $a_{2}^{(3)}=(h_{\Theta}(x))_{2}$
\end_inset

 （注意
\begin_inset Formula $\Theta$
\end_inset

要用大写）be the activation of the first output unit, and similarly 
\begin_inset Formula $a_{2}^{\left(3\right)}=(h_{\Theta}(x))2$
\end_inset

 and 
\begin_inset Formula $a_{3}^{(3)}=(h_{\Theta}(x))_{3}$
\end_inset

.
 Then for any input x, it must be the case that 
\begin_inset Formula $a_{1}^{(3)}+a_{2}^{(3)}+a_{3}^{(3)}=1$
\end_inset

。答案：错误，因为输出并非是概率，所以输出的总和并不一定是1。
\end_layout

\begin_layout Section
Neural Networks: Learning(the parameters)
\end_layout

\begin_layout Subsection
Cost Function
\end_layout

\begin_layout Subsubsection
Neural Network (Classification)
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 9.1.Cost Function.png
	scale 75

\end_inset


\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Neural-Network-(Classification)"

\end_inset

Neural Network (Classification)
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Neural-Network-(Classification)"

\end_inset

解释：
\end_layout

\begin_layout Standard
L: 网络层数。
\end_layout

\begin_layout Standard
\begin_inset Formula $s_{l}$
\end_inset

: 第l层的节点个数（不包括bias节点）。
\end_layout

\begin_layout Standard
k: output layer 的节点个数。
\end_layout

\begin_layout Standard
我们将考虑binary classification 和 Muti-class classification(
\begin_inset Formula $K\geqslant3$
\end_inset

，
\begin_inset Formula $K=2$
\end_inset

没意义) 两种情况。
\end_layout

\begin_layout Subsubsection
Cost function
\end_layout

\begin_layout Paragraph
Logistic regression:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
J\left(\theta\right)=-\frac{1}{m}\left[\sum_{i=1}^{m}y^{\left(i\right)}\log h_{\theta}\left(x^{\left(i\right)}\right)+\left(1-y^{\left(i\right)}\right)\log\left(1-h_{\theta}\left(x^{\left(i\right)}\right)\right)\right]+\frac{\lambda}{2m}\sum_{j=1}^{n}\theta_{j}^{2}\label{eq:cost func logistic regression with reg2}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
The cost function we use for the neural network is going to be the generalizatio
n of the one that we use for logistic regression(见公式
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:cost func logistic regression with reg"

\end_inset

和
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:cost func logistic regression with reg2"

\end_inset

).
 
\end_layout

\begin_layout Paragraph
Neural network:(有K个输出)
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
h_{\Theta}\left(x\right)\in\mathbb{R}^{K}\qquad\left(h_{\Theta}\left(x\right)\right)_{i}=i^{th}\, output
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\begin{aligned}J\left(\Theta\right)=-\frac{1}{m}\left[\sum_{i=1}^{m}\sum_{k=1}^{K}y_{k}^{\left(i\right)}\log\left(h_{\Theta}\left(x^{\left(i\right)}\right)\right)_{k}+\left(1-y_{k}^{\left(i\right)}\right)\log\left(1-\left(h_{\Theta}\left(x^{\left(i\right)}\right)\right)_{k}\right)\right]\\
+\frac{\lambda}{2m}\sum_{l=1}^{L-1}\sum_{i=1}^{s_{l}}\sum_{j=1}^{s_{l+1}}\left(\Theta_{ji}^{\left(l\right)}\right)^{2}
\end{aligned}
\label{eq:cost function of neural network}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
For logistic regression, we used to minimize the cost function j of theta
 that was shown above.
 For neural network, instead of having basically just one logistic regression
 output unit, we have 
\series bold
K of them
\series default
.
\end_layout

\begin_layout Standard
In the cost funtion, J of 
\begin_inset Formula $\theta$
\end_inset

, we have a sum from k = 1~K.
 This is basically the sum over the K output unit.
 It's basically the logistic regression algorithm's cost function but 
\series bold
summing
\series default
 that cost function over each of my output units in turn.
\end_layout

\begin_layout Standard
因此在公式
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:cost function of neural network"

\end_inset

中，我们使用了
\begin_inset Formula $y_{k}$
\end_inset

和
\begin_inset Formula $h_{\Theta}\left(\ldots\right)_{k}$
\end_inset

来表示针对每一个输出项的y和hypothesis。虽然该公式的regularization项看起来非常复杂，但无非是在计算除了bias unit（i=0）外所有
的
\begin_inset Formula $\left(\Theta_{ji}^{\left(l\right)}\right)^{2}$
\end_inset

的总和而已，和logistic regression一样，从1开始（去除bisa unit）而不是从0开始仅仅是个惯例，即使从0开始也不会有太大的不同。
\end_layout

\begin_layout Subsection
Backpropagation Algorithm (to compute derivation of cost function)
\end_layout

\begin_layout Subsubsection
Gradient Comutation
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 9.2.backpropagation algorithm.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Gradient-computation"

\end_inset

Gradient computation
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
如图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Gradient-computation"

\end_inset

，What we want to do is to focus on how we can compute the 
\series bold
partial derivative 
\series default
terms.
 
\end_layout

\begin_layout Standard
让我们从只有一个training example 
\begin_inset Formula $\left(x,y\right)$
\end_inset

 的情况开始，如图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Forward-propagation过程"

\end_inset

所示，逐步计算cost function的偏导。第一步为forward propagation，用来计算hypothesis输出。
\begin_inset Foot
status open

\begin_layout Plain Layout
在设计神经网络时，我们自然是没有现成的矩阵的（我们的任务也就是计算
\begin_inset Formula $\Theta$
\end_inset

矩阵），然而却需要先用前向传播算法计算Hypothesis(这势必要用到
\begin_inset Formula $\Theta$
\end_inset

)。虽然课程未做说明，但这里的
\begin_inset Formula $\Theta$
\end_inset

往往是随机生成的值，具体可以参考第
\begin_inset CommandInset ref
LatexCommand ref
reference "sub:Random-Initialization"

\end_inset

节和
\begin_inset CommandInset ref
LatexCommand ref
reference "sub:Putting-It-Together"

\end_inset

节。这里只是提醒一下，以免看到相关内容时会困惑。
\end_layout

\end_inset


\end_layout

\begin_layout Standard
貌似图中的 
\begin_inset Formula $a^{\left(1\right)}$
\end_inset

应该加上bias unit（
\begin_inset Formula $a_{0}^{\left(1\right)}$
\end_inset

）。
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 9.2.backpropagation algorithm2.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Forward-propagation过程"

\end_inset

Forward propagation过程
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
要计算偏导，下一步需要用到后向传播（backpropagation）算法。对该算法的直观理解就是对每个节点计算一个
\begin_inset Formula $\delta_{j}^{\left(l\right)}$
\end_inset

，表示节点
\begin_inset Formula $j$
\end_inset

在第
\begin_inset Formula $l$
\end_inset

层的“误差”（error）。比如要计算输出层节点的“误差”，如图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:back-propagation intuition"

\end_inset

，那么只需计算每个节点
\begin_inset Formula $a_{j}^{\left(4\right)}$
\end_inset

和结果
\begin_inset Formula $y_{i}$
\end_inset

的差距即可。
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 9.2.backpropagation algorithm3.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:back-propagation intuition"

\end_inset

backpropagation intuition
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
注意图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:back-propagation intuition"

\end_inset

中的算式可以矢量化为 
\begin_inset Formula $\delta^{\left(4\right)}=a^{\left(4\right)}-y=\left(h_{\Theta}\left(x\right)\right)-y$
\end_inset

 
\end_layout

\begin_layout Standard
下一步需要计算前面每一层的
\begin_inset Formula $\delta$
\end_inset

，见式
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:compute delta for 3rd layer"

\end_inset

、
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:compute delta for 2nd layer"

\end_inset

。注意式
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:compute delta for 3rd layer"

\end_inset

中
\begin_inset Formula $\left(\Theta^{\left(3\right)}\right)^{T}\delta^{\left(4\right)}$
\end_inset

是一个矢量，
\begin_inset Formula $g^{\prime}\left(z^{\left(3\right)}\right)$
\end_inset

也是一个矢量，他们之间使用的是点乘号（element wise），表示按元素进行乘法运算（两矢量的对应元素分别相乘）。
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
\begin_inset Formula $z^{\left(3\right)}$
\end_inset

是5*1的矢量，然而如果按之前
\begin_inset Formula $\Theta$
\end_inset

的定义，那么
\begin_inset Formula $\left(\Theta^{\left(3\right)}\right)^{T}\delta^{\left(4\right)}$
\end_inset

的结果应该是6*1的。因此这里的
\begin_inset Formula $\Theta$
\end_inset

应该不包括bias unit，即
\begin_inset Formula $\Theta$
\end_inset

是4*5的，这样结果就是5*1的了。编程训练中也是这样处理的。
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\delta^{\left(3\right)}=\left(\Theta^{\left(3\right)}\right)^{T}\delta^{\left(4\right)}\,.*g^{\prime}\left(z^{\left(3\right)}\right)\label{eq:compute delta for 3rd layer}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\delta^{\left(2\right)}=\left(\Theta^{\left(2\right)}\right)^{T}\delta^{\left(3\right)}\,.*g^{\prime}\left(z^{\left(2\right)}\right)\label{eq:compute delta for 2nd layer}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
这里，
\begin_inset Formula $g^{\prime}\left(z^{\left(3\right)}\right)$
\end_inset

 是activation function 
\begin_inset Formula $g\left(z^{\left(3\right)}\right)$
\end_inset

的导数:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
g^{\prime}\left(z^{\left(3\right)}\right)=a^{\left(3\right)}\,.*\left(1-a^{\left(3\right)}\right)\label{eq:partial derivative of cost func in NN without reg}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
这里不对该式的得出进行推导（我没看懂是怎么得出来的）。
\begin_inset CommandInset label
LatexCommand label
name "why:sigmoid函数求导的推导"

\end_inset


\end_layout

\begin_layout Standard
注意没有 
\begin_inset Formula $\delta_{1}$
\end_inset

这一说。
\end_layout

\begin_layout Standard
The name backpropagation comes from the fact that we start by computing
 the delta term for the output layer and then we 
\series bold
go back
\series default
 a layer and compute the delta terms for that layer.
 So we are back propagating the errors from the current layer to the earlier
 layer.
\end_layout

\begin_layout Standard
最后得到的求偏导的公式非常复杂，可以证明（过程比这个公式还要复杂，故略掉），如果忽略regularization(
\begin_inset Formula $\lambda=0$
\end_inset

), 那么我们需要的偏导数是：
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\frac{\partial}{\partial\Theta_{ij}^{\left(l\right)}}J\left(\Theta\right)=\delta_{i}^{\left(l+1\right)}a_{j}^{\left(l\right)}\label{eq:partial derivative of cost func of logistic in NN without regularization}
\end{equation}

\end_inset


\end_layout

\begin_layout Subsubsection
Backpropagation algorithm
\end_layout

\begin_layout Standard
如果training example不止一个，在这里设定有m个，如图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Backward-propagation-algorithm"

\end_inset

所示，首先设置
\begin_inset Formula $\Delta_{ij}^{\left(l\right)}=0$
\end_inset

(for all l, i, j)，这里的
\begin_inset Formula $\Delta$
\end_inset

是用来计算cost function的偏导的，而且正如即将看到的，
\begin_inset Formula $\Delta$
\end_inset

其实是个累加数(Accumulator)。针对每个training example执行图中所示循环，每次循环都更新一次
\begin_inset Formula $\Delta$
\end_inset

。
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 9.2.backpropagation algorithm4.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Backward-propagation-algorithm"

\end_inset

Backpropagation algorithm
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Foot
status open

\begin_layout Plain Layout
图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Backward-propagation-algorithm"

\end_inset

中计算
\begin_inset Formula $D_{ij}^{\left(l\right)}$
\end_inset

的公式错误。应该为 
\begin_inset Formula $D_{ij}^{\left(l\right)}:=\frac{1}{m}\left(\Delta_{ij}^{\left(l\right)}+\lambda\Theta_{ij}^{\left(l\right)}\right)\qquad if\; j\neq0$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
也如图中所示，
\begin_inset Formula $\Delta$
\end_inset

的计算可以矢量化为
\begin_inset Formula $\Delta^{\left(l\right)}:=\Delta^{\left(l\right)}+\delta^{\left(l+1\right)}\left(a^{\left(l\right)}\right)^{T}$
\end_inset

的形式。
\end_layout

\begin_layout Standard
在循环外计算
\begin_inset Formula $D_{ij}^{\left(l\right)}$
\end_inset

，分为
\begin_inset Formula $j=0$
\end_inset

（bias unit）和
\begin_inset Formula $j\neq0$
\end_inset

两种情况。
\end_layout

\begin_layout Standard
可以证明，最后求得的
\begin_inset Formula $D_{ij}^{\left(l\right)}$
\end_inset

就是cost function的偏导：
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\frac{\partial}{\partial\Theta_{ij}^{\left(l\right)}}J\left(\Theta\right)=D_{ij}^{\left(l\right)}\label{eq:partial derivative of cost func of logistic in NN with regularization}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
课堂习题（图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:课堂练习：计算gradient步骤"

\end_inset

）的答案就是求gradient过程的简单概括。
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 9.2.backpropagation algorithm5.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:课堂练习：计算gradient步骤"

\end_inset

课堂练习：计算gradient步骤
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Implementation note: Unrolling parameters
\end_layout

\begin_layout Standard
我们需要用到的函数一般都将
\begin_inset Formula $\theta$
\end_inset

视为矢量，而神经网络中的
\begin_inset Formula $\Theta$
\end_inset

是矩阵，因此需要将矩阵转化为矢量后才能使用之前介绍过的函数（如fminunc）进行计算（图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Need-to-unrol-params"

\end_inset

）。
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 9.3.Unrolling parameters.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Need-to-unrol-params"

\end_inset

Need to unrol parameters
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 9.3.Unrolling parameters2.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:矩阵和矢量的互转"

\end_inset

矩阵和矢量的互转
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:矩阵和矢量的互转"

\end_inset

所示便是矩阵到矢量和矢量到矩阵的转换方法。 
\end_layout

\begin_layout Subsection
Gradient Checking
\end_layout

\begin_layout Standard
神经网络的后向算法可能会出现很多微妙的错误，因此最好在投入运作之前检查一下算法是否工作正常。
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 9.4.Gradient Checking.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Gradient错误检查"

\end_inset

Gradient错误检查
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Gradient错误检查"

\end_inset

是假设
\begin_inset Formula $\theta$
\end_inset

是一个实数（
\begin_inset Formula $\theta\in\mathbb{R}$
\end_inset

）的情况，我们通过检查斜率和我们估计的斜率值的误差是否在容许范围之内来判断算法是否工作正常。当
\begin_inset Formula $\theta$
\end_inset

是矢量（
\begin_inset Formula $\theta\in\mathbb{R}$
\end_inset

）时，我们计算的近似斜率如图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:近似斜率的计算"

\end_inset

：
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 9.4.Gradient Checking2.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:近似斜率的计算"

\end_inset

近似斜率的计算
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
在Octave中实现如图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Octave实现的近似斜率"

\end_inset

：
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 9.4.Gradient Checking3.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Octave实现的近似斜率"

\end_inset

Octave实现的近似斜率
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
实现提示：检查无误后，记得关掉Gradient checking(time costing)。相比后向传播，Gradient计算是非常耗时的（图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:使用后向传播而不是Gradient-descent的原因"

\end_inset

）。
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 9.4.Gradient Checking4.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:使用后向传播而不是Gradient-descent的原因"

\end_inset

使用后向传播而不是Gradient descent的原因
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Random Initialization
\begin_inset CommandInset label
LatexCommand label
name "sub:Random-Initialization"

\end_inset


\end_layout

\begin_layout Subsubsection
Initial value of 
\begin_inset Formula $\Theta$
\end_inset


\end_layout

\begin_layout Standard
For gradient descent and advanced optimization method, need initial value
 for 
\begin_inset Formula $\Theta$
\end_inset

.
\end_layout

\begin_layout Standard
把
\begin_inset Formula $\Theta$
\end_inset

初始化为零向量（在logistic regression中就是这么做的）在 神经网络中是行不通的，见图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:将初始化为0的问题"

\end_inset

，在每次更新（gradient descent, etc）后，每个参数的权重永远是相同的。有时这被称作 The problem of symmetric
 weights.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 9.5.Random Init.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:将初始化为0的问题"

\end_inset

将
\begin_inset Formula $\Theta$
\end_inset

初始化为0将导致每一层元素全部相同
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Random initialization: Symmetry(对称) breaking
\end_layout

\begin_layout Standard
将每一个
\begin_inset Formula $\Theta_{ij}^{\left(l\right)}$
\end_inset

初始化为
\begin_inset Formula $\left[-\epsilon,\epsilon\right]$
\end_inset

之间的随机值：
\end_layout

\begin_layout LyX-Code
\align center
Theta1 = rand(10,11)*(2*INIT_EPSILON) - INIT_EPSILON;
\end_layout

\begin_layout Subsection
Putting It Together
\begin_inset CommandInset label
LatexCommand label
name "sub:Putting-It-Together"

\end_inset


\end_layout

\begin_layout Subsubsection
Training a neural network
\end_layout

\begin_layout Enumerate
The first thing you need to do is to pick some network architecture:
\end_layout

\begin_deeper
\begin_layout Itemize
Number of input units: Dimension of features 
\begin_inset Formula $x^{\left(i\right)}$
\end_inset

.
\end_layout

\begin_layout Itemize
Number of output units: Number of classes.
\end_layout

\begin_layout Itemize
Reasonable default: 1 hidden layer, or if >1 hidden layer, have same number
 of hidden units in every layer.
\end_layout

\begin_layout Itemize
Usually the number of hidden units in each layer will be comparable to the
 dimension of 
\begin_inset Formula $x$
\end_inset

.
 Same number or 3 or 4 times of that(usually the more the better).
\end_layout

\end_deeper
\begin_layout Enumerate
Randomly initialize weights.
\end_layout

\begin_layout Enumerate
Implement forward propagation to get 
\begin_inset Formula $h_{\Theta}\left(x^{\left(i\right)}\right)$
\end_inset

 for any 
\begin_inset Formula $x^{\left(i\right)}$
\end_inset

.
\end_layout

\begin_layout Enumerate
Implement code to compute cost function 
\begin_inset Formula $J\left(\Theta\right)$
\end_inset

.
\end_layout

\begin_layout Enumerate
Implement backprop to compute partial derivatives 
\begin_inset Formula $\frac{\partial}{\partial\Theta_{jk}^{\left(l\right)}}J\left(\Theta\right)$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 9.6.put together.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:backpropagation-contour"

\end_inset

backpropagation contour
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:backpropagation-contour"

\end_inset

为后向传播算法概述，完整算法见 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Backward-propagation-algorithm"

\end_inset

。
\end_layout

\end_deeper
\begin_layout Enumerate
Use gradient checking to compare 
\begin_inset Formula $\frac{\partial}{\partial\Theta_{jk}^{\left(l\right)}}J\left(\Theta\right)$
\end_inset

 computed using backpropagation vs.
 using numerical estimate of gradient of 
\begin_inset Formula $J\left(\Theta\right)$
\end_inset

.
\begin_inset Newline newline
\end_inset

Then disable gradient checking code.
\end_layout

\begin_layout Enumerate
Use gradient descent or advanced optimization method with backpropatation(
 to compute 
\begin_inset Formula $\frac{\partial}{\partial\Theta_{jk}^{\left(l\right)}}J\left(\Theta\right)$
\end_inset

 ) to try to minimize 
\begin_inset Formula $J\left(\Theta\right)$
\end_inset

 as a function of parameters 
\begin_inset Formula $\Theta$
\end_inset

.
\end_layout

\begin_layout Standard

\series bold
注意
\series default
：在神经网络中，
\begin_inset Formula $J\left(\Theta\right)$
\end_inset

 是 non-convex 的。所以s理论上可能会得到局部最优解。不过在实践中这并不是一个大问题，通常得到的解已足够令人满意。
\end_layout

\begin_layout Standard
Gradient descent 的直观描述见图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:回顾Gradient-descent"

\end_inset

。
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 9.6.put together2.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:回顾Gradient-descent"

\end_inset

回顾Gradient descent
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
What gradient descent does is starting from some random initial point, and
 it will repeatedly go downhill.
 And so what backpropagation is doing is computing the direction of the
 gradient, and what gradient descent is doing is taking little steps downhill
 untill hopefully it gets to a pretty good local optimum.
\end_layout

\begin_layout Standard
When you implement backpropagation and use gradient descent or one of the
 advanced optimization methods, this picture sort of explaining what the
 algorithm is doing.
\end_layout

\begin_layout Section
Advice for applying machine learning
\end_layout

\begin_layout Subsection
Deciding what to try next
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 10.1.Deciding what to do next.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:What-to-try-next"

\end_inset

What to try next
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
如图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:What-to-try-next"

\end_inset

，在之后的课程里，将会介绍Machine learning diagnostic.
\end_layout

\begin_layout Standard
It's a test that you can run to gain insight what is/isn't working with
 a learning algorithm, and gain guidance as to how best to improve its performan
ce.
\end_layout

\begin_layout Subsection
Evaluating a hypothesis
\end_layout

\begin_layout Standard
我们可以把hypothesis画出来，判断是否出现overfitting 或 underfitting.
 可如果features 数量很多的话，画图就不是什么好主意了。
\end_layout

\begin_layout Standard
一般化的方法是将dataset 分割为两部分：第一部分用作training set, 而第二部分用来做test set。分割的比例大概是7:3。在分割前先对da
taset随机排序一下（如果其排列不是随机的话）。
\end_layout

\begin_layout Subsubsection
Training/testing procedure for linear regression
\end_layout

\begin_layout Standard
见图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Training/testing-for-linear-gregression"

\end_inset

。
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 10.2.procedure for linear regression.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Training/testing-for-linear-gregression"

\end_inset

Training/testing procedure for linear regression
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Training/testing procedure for logistic regression(classification)
\end_layout

\begin_layout Standard
图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Training/testing-procedure-for-classification"

\end_inset

给出了两种检验学习质量的方法，一种是使用cost function，另一种称为misclassification error。不论使用哪种方法，如果算出的误差偏
大，那么说明机器学习的质量偏低。
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 10.2.procedure for logistic regression.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Training/testing-procedure-for-classification"

\end_inset

Training/testing procedure for classification
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Model selection and training/validation/test sets
\begin_inset CommandInset label
LatexCommand label
name "sub:Model-selection-and-training/validation/test sets"

\end_inset


\end_layout

\begin_layout Standard
Suppose you'd like to decide what the degree of polynomial to fit to a data
 set, sort of what features to include to give you a learning algorithm.
 Or suppose you'd like to choose the regularization parameter 
\begin_inset Formula $\lambda$
\end_inset

 for the learning algorithm.
 These are called 
\series bold
model selection problems
\series default
.
\end_layout

\begin_layout Standard
在model selection 问题中，我们将把data set分成trainig、validation 和 test 三部分，而不只是 training
 和 test 两部分。
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 10.3.model selection.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Overfitting-example"

\end_inset

Overfitting example
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 10.3.model selection2.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Model-slection"

\end_inset

Model slection
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
如图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Model-slection"

\end_inset

所示，假定我们列出d=1到d=10（d 表示多项式的维度）10个模型，然后计算每个模型的测试误差(test error)
\begin_inset Formula $J_{test}\left(\theta\right)$
\end_inset

，最后选择测试误差最小的模型。假设我们最终选择的模型是d=5的多项式，事情进行到这里都是很正常的。然而现在我想知道我选择的hypothesis模型的归纳质量（H
ow well this model generalized）。一种方法是使用test error来衡量，然而问题是使用 
\begin_inset Formula $J_{test}\left(\theta\right)$
\end_inset

来估计模型的归纳（generalization）质量是不公平的，因为What we done is we fit the extra parameter
 
\begin_inset Formula $d$
\end_inset

 using the test set，即选择模型时就是以test set error作为标准，如果再用test set error来衡量hypothesis的
话，那该模型的 
\begin_inset Formula $\Theta$
\end_inset

 可能过于乐观的估计了它的归纳质量。 也就是说，建立在 test set 上的估计是不全面的。
\end_layout

\begin_layout Standard
那么我们就将数据分成三部分： Training set, Cross validation set 和 Test set，大概是6:2:2的比例，这样就避免了在
模型选择和模型评估中使用同一个数据集合的问题了。见图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Evaluating-hypothesis"

\end_inset

。
\end_layout

\begin_layout Subsubsection
Evaluating your hypothesis
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 10.3.model selection3.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Evaluating-hypothesis"

\end_inset

Evaluating hypothesis
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Train/validation/test error
\end_layout

\begin_layout Standard
Training error:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
J_{train}\left(\theta\right)=\frac{1}{2m}\sum_{i=1}^{m}\left(h_{\theta}\left(x^{\left(i\right)}\right)-y^{\left(i\right)}\right)^{2}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Cross Validation error:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
J_{cv}\left(\theta\right)=\frac{1}{2m_{cv}}\sum_{i=1}^{m_{cv}}\left(h_{\theta}\left(x_{cv}^{\left(i\right)}\right)-y_{cv}^{\left(i\right)}\right)^{2}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Test error:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
J_{test}\left(\theta\right)=\frac{1}{2m_{test}}\sum_{i=1}^{m_{test}}\left(h_{\theta}\left(x_{test}^{\left(i\right)}\right)-y_{test}^{\left(i\right)}\right)^{2}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
So when we fits the model selection problem by this, instead of using the
 test set to select the model, we're instead going to use validation set
 or the cross-validation set to select the model.
 见图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:改用cross-validation error选择模型"

\end_inset

。
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 10.3.model selection4.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:改用cross-validation error选择模型"

\end_inset

改用cross-validation set error选择hypothesis模型
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
课堂习题（图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:本节课后题"

\end_inset

）也许能让我们对这个问题的理解更加清晰，我们选择的是cross-validation值最小的模型，因此
\begin_inset Formula $J_{CV}\left(\theta\right)$
\end_inset

往往比
\begin_inset Formula $J_{test}\left(\theta\right)$
\end_inset

小。
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename 10.3.model selection5.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:本节课后题"

\end_inset

本节课堂习题
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 10.3.excercise.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
有关Train/test方法的一个课后题解释
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Diagnosing bias vs.
 variance
\end_layout

\begin_layout Standard
If you run the learning algorithm and it doesn't do as well as you're hoping,
 almost all the time it will be because you have either a 
\series bold
high bias
\series default
 problem or a 
\series bold
high variance
\series default
 problem.
 In other words they're either an 
\series bold
underfitting
\series default
 problem or an 
\series bold
overfitting
\series default
 problem.
\end_layout

\begin_layout Standard
如果发现模型的交叉验证误差(Cross-validation error)或测试误差(Test error)偏大，那么如何知道我们的模型是拟合度太低(high
 bias)还是太高(high variance)？答案是根据训练集误差
\begin_inset Formula $J_{train}\left(\theta\right)$
\end_inset

，见图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:模型拟合问题诊断(bias还是variance)"

\end_inset

所述。
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 10.4.bias vs variance.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:模型拟合问题诊断(bias还是variance)"

\end_inset

模型拟合问题诊断(bias还是variance)
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Regularization and bias/variance
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 10.5.regularization.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Regularization-and-bias/variance"

\end_inset

Regularization and bias/variance
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
How can we automatically choose a good value for the regularization parameter
 
\begin_inset Formula $\lambda$
\end_inset

 ?
\end_layout

\begin_layout Subsubsection
Choosing the regularization parameter 
\begin_inset Formula $\lambda$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 10.5.regularization2.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:只有J(theta)有参数"

\end_inset

只有
\begin_inset Formula $J\left(\theta\right)$
\end_inset

有
\begin_inset Formula $\lambda$
\end_inset

参数
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
如图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:只有J(theta)有参数"

\end_inset

所示，
\begin_inset Formula $J_{train}\left(\theta\right)$
\end_inset

、
\begin_inset Formula $J_{cv}\left(\theta\right)$
\end_inset

和
\begin_inset Formula $J_{test}\left(\theta\right)$
\end_inset

 都没有进行regularization。
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 10.5.regularization3.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:选择合适的lambda"

\end_inset

选择合适的
\begin_inset Formula $\lambda$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
如图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:选择合适的lambda"

\end_inset

所示，针对选定的模型，测试不同的
\begin_inset Formula $\lambda$
\end_inset

取值，最后选择一个使Cross validation error最小的 
\begin_inset Formula $\lambda$
\end_inset

 即可。选定之后，计算该 
\begin_inset Formula $\theta$
\end_inset

 的test error: 
\begin_inset Formula $J_{test}\left(\Theta\right)$
\end_inset

 来检查其在test set 上的表现如何。
\end_layout

\begin_layout Subsubsection
Bias/variance as a function of the regularization parameter 
\begin_inset Formula $\lambda$
\end_inset


\end_layout

\begin_layout Standard
见图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Bias/variance-as-a-func-of-lambda-2"

\end_inset

和图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Bias/variance-as-a-func-of-lambda-2"

\end_inset

。
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 10.5.regularization4.png

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Bias/variance-as-a-func-of-lambda"

\end_inset

Bias/variance as a function of the regularization parameter 
\begin_inset Formula $\lambda$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 10.5.regularization5.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Bias/variance-as-a-func-of-lambda-2"

\end_inset

Bias/variance as a function of the regularization parameter 
\begin_inset Formula $\lambda$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Learning curves
\end_layout

\begin_layout Standard
学习曲线是一种非常有用的图，比如用来检查算法是否正常工作，或者用来提升算法的性能。事实上学习曲线是一个有力的工具，可以用来诊断学习算法是否正遭受bias、var
iance或两者。图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Learning-curves"

\end_inset

展示了学习曲线的样子。
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 10.6.Learning curves.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Learning-curves"

\end_inset

Learning curves
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
正如图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Learning-curves"

\end_inset

所示，我们限制训练集的大小，以训练集元素个数为自变量，误差为因变量，这样便构成了学习曲线。注意，不管是训练集的学习曲线还是交叉验证集的学习曲线，自变量都是训练集
的元素尺寸。
\end_layout

\begin_layout Subsubsection
High bias
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 10.6.Learning curves high bias.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Learning-curve-of-high-bias"

\end_inset

Learning curve of high bias
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
如果学习算法拟合度过低（high bias），如图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Learning-curve-of-high-bias"

\end_inset

，那么增加训练集容量是徒劳的，Both 
\begin_inset Formula $J_{cv}\left(\Theta\right)$
\end_inset

 and 
\begin_inset Formula $J_{train}\left(\Theta\right)$
\end_inset

 are high.
\end_layout

\begin_layout Subsubsection
High variance
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 10.6.Learning curves high variance.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Learning-curve-of-high-variance"

\end_inset

Learning curve of high variance
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
如图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Learning-curve-of-high-variance"

\end_inset

，如果学习算法拟合度过高（high variance），那么增大训练集容量应该会有帮助。
\end_layout

\begin_layout Subsection
Deciding what to try next (revisited)
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 10.7.revisited.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Debugging-a-learning-algorithm"

\end_inset

Debugging a learning algorithm
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
如图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Debugging-a-learning-algorithm"

\end_inset

，当学习算法效果不理想时，我们可以采用很多方法来改进算法：
\end_layout

\begin_layout Itemize
使用更大的训练集合。这可以改善拟合过度的问题。
\end_layout

\begin_layout Itemize
精简贡献小的features以减少features的数量。这可以改善拟合过度的问题。
\end_layout

\begin_layout Itemize
寻找可能的features以扩大features的数量。这可以改善拟合不够的问题。
\end_layout

\begin_layout Itemize
增加多项式features(如添加
\begin_inset Formula $x_{1}^{2},\, x_{2}^{2},\, x_{1}x_{2}$
\end_inset

等)。这可以改善拟合不够的问题。
\end_layout

\begin_layout Itemize
减小
\begin_inset Formula $\lambda$
\end_inset

。这可以改善拟合不够的问题。
\end_layout

\begin_layout Itemize
增大
\begin_inset Formula $\lambda$
\end_inset

。这可以改善拟合过度的问题。
\end_layout

\begin_layout Subsubsection
Neural networks and overfitting
\end_layout

\begin_layout Standard
如果要改变神经网络的层数，可以测试一下不同层数神经网络的
\begin_inset Formula $J_{cv}\left(\Theta\right)$
\end_inset

 以找到最佳结果。
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 10.7.neural network.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:神经网络中的模型选择问题"

\end_inset

神经网络中的模型选择问题
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 10.7.neural network2.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:本节课上习题"

\end_inset

本节课上习题
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
如本节课上习题（图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:本节课上习题"

\end_inset

）给出的情景：若交叉验证误差远大于训练集误差时，那这应该是拟合过度的情况，增加隐藏层单位只能让事情更遭。
\end_layout

\begin_layout Section
Machine Learning System Design
\end_layout

\begin_layout Subsection
Prioritizing what to work on: Spam classification example
\end_layout

\begin_layout Standard
采用 Supervised leaning 来构建一个垃圾邮件分类器，问题就在于如何选择 email 的features，见图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:垃圾邮件分类器的构建思路"

\end_inset

，比如让features (
\begin_inset Formula $x$
\end_inset

)是100个对垃圾邮件有区分作用（真或假）的单词，而输出y则是 spam(1) 或 not spam(0)两个值。
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 11.1 Spam classifier.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:垃圾邮件分类器的构建思路"

\end_inset

垃圾邮件分类器的构建思路
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
Note: In practice, take most frequently occurring 
\begin_inset Formula $n$
\end_inset

 words (10,000 to 50,000) in training set, rather than manually pick 100
 words.
\end_layout

\begin_layout Standard

\series bold
How to spend your time to make it have low error?
\end_layout

\begin_layout Standard
Andrew Ng老师给我们提出4个建议（图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:有助于降低算法误差的建议"

\end_inset

）。
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 11.1 Spam classifier2.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:有助于降低算法误差的建议"

\end_inset

有助于降低垃圾邮件算法误差的建议
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
本节可后习题见图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:课上习题：关于垃圾邮件判别算法"

\end_inset

。
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 11.1 Spam classifier3.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:课上习题：关于垃圾邮件判别算法"

\end_inset

课上习题：关于垃圾邮件判别算法
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Error analysis
\end_layout

\begin_layout Subsubsection
Recommended approach
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 11.2.error analysing.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:设计算法的几个建议"

\end_inset

设计算法的几个建议
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:设计算法的几个建议"

\end_inset

为Ng老师给出的关于设计学习算法的几个有用的建议，而且Ng老师推荐先用24小时的时间构建一个相当简单而且脏乱的系统。然后使用cross-validation
 data来测试。之后通过画学习曲线和误差分析来优化系统。
\end_layout

\begin_layout Subsubsection
Error Analysis
\end_layout

\begin_layout Standard
见图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Error-analysis"

\end_inset

，通过手动分析算法在交叉验证集上出现的差错，这有助于对现有算法进行改进，比如添加修改feature等，提高侦测率。
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 11.2.error analysing2.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Error-analysis"

\end_inset

Error analysis
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
The importance of numerical evaluation
\end_layout

\begin_layout Standard
正如前文（第
\begin_inset CommandInset ref
LatexCommand ref
reference "sub:Model-selection-and-training/validation/test sets"

\end_inset

节）所示，测试集只是为了检验模型是否有良好的归纳质量（generalization）而存在，如果用在模型选择中，那么再次用测试集对模型归纳质量进行估计是不公平的
，见课上习题（图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Reason-to-use-cv-rather-than-test"

\end_inset

）。
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 11.2.error analysing3.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Reason-to-use-cv-rather-than-test"

\end_inset

Reason to use cross validation data rather than test data
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
误差分析（Error analysis）不是万能的，很多情况下需要我们手动实验才能知道。（见图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Try-&-See"

\end_inset

）
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 11.2.error analysing4.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Try-&-See"

\end_inset

Try & See
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
最后再啰嗦一次：
\series bold
强烈建议
\series default
使用 cross validation error 而不是 test error 来进行误差分析。
\end_layout

\begin_layout Subsection
Error metrics for skewed(歪斜的) classes
\end_layout

\begin_layout Subsubsection
Cancer classification example
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 11.3.Skewed class.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:歪斜分布问题——癌症诊断"

\end_inset

歪斜分布问题——（不负责任的）癌症诊断
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
在skewed class（样本中某一类别出现的频率非常之小）的情况下，使用分类准确率衡量算法质量将变得非常困难（比如图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:歪斜分布问题——癌症诊断"

\end_inset

，将诊断正确率从99.2%提升到99.5%，谁知道是不是算法更先进了还是单纯的预测出更多的y=0？）
\end_layout

\begin_layout Standard
对于skewed class问题，我们可能需要其他测试方式，这里将介绍 error metric / evaluation metric。
\end_layout

\begin_layout Subsubsection
Precision/Recall（查准率/查全率）
\end_layout

\begin_layout Description
Precision
\begin_inset space ~
\end_inset

&
\begin_inset space ~
\end_inset

recall In pattern recognition (模式识别) and information retrieval (信息检索), precision
 is the fraction of retrieved instances that are relevant, while recall
 is the fraction of relevant instances that are retrieved.
\begin_inset Foot
status open

\begin_layout Plain Layout
引用自英文维基百科：
\begin_inset CommandInset href
LatexCommand href
name "Precision and recall"
target "http://en.wikipedia.org/wiki/Precision_and_recall"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
比如在信息检索领域：
\end_layout

\begin_layout Standard
\align center
查准率＝检索出的相关信息量/检索出的信息总量
\end_layout

\begin_layout Standard
\align center
查全率＝检索出的相关信息量/系统中的相关信息总量
\end_layout

\begin_layout Standard
查准率(precision)是衡量检索系统和检索者拒绝非相关信息的能力，查全率(recall)是衡量检索系统和检索者检出相关信息的能力。
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 11.3.Precision Recall.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 11.3.Precision Recall2.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Precision-&-Recall"

\end_inset

Precision & Recall
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
详见图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Precision-&-Recall"

\end_inset

，在癌症诊断的例子中，查准率是指在诊断为癌症的人里面有多少是真的癌症，而查全率是指真正患癌症的人中有多少被诊断了出来。Precision 和 recall
 的取值都是越高越有利。特别地，如果像上一张幻灯片（图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:歪斜分布问题——癌症诊断"

\end_inset

）那样诊断所有病人的结果都为良性（0），那么将得到值为0的precision和recall，从而有效的避免了这种不负责任的诊断方式。
\end_layout

\begin_layout Standard
所以，如果我们得到了很高的 precision 和 recall ，那么我们可以信任我们的学习算法，即使样本是非常“歪斜”的情况。
\end_layout

\begin_layout Subsection
Trading off precision and recall
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 11.4.Trading off precision and recall.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:权衡precision和recall-提高precision"

\end_inset

权衡precision和recall——提高查准率
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
当我们将癌症诊断阈值从0.5改成更高的值，以达到只有当我们非常自信时才预测为癌症，那么这会带来更高的precision，和更低的recall，如图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:权衡precision和recall-提高precision"

\end_inset

所示。
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 11.4.Trading off precision and recall2.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:权衡precision和recall——提高查全率"

\end_inset

权衡precision和recall——提高查全率
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
而如果我们宁愿错诊也不想让病人错过治疗，那么 就需要调低诊断阈值，这样就会带来更高的recall和更低的precision，如图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:权衡precision和recall——提高查全率"

\end_inset

所示。
\end_layout

\begin_layout Subsubsection
\begin_inset Formula $F_{1}$
\end_inset

 Score (F score)
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 11.4.Trading off precision and recall3.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:不能用Precision和Recall的平均代替这两个值"

\end_inset

不能用Precision和Recall的平均代替这两个值
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
现在一个学习算法有两个变量（precision和recall）来描述，我们希望使用一个单值来衡量一个学习算法。如图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:F-score"

\end_inset

所示，使用平均值是相当没用的方式。这里我们使用 
\begin_inset Formula $F_{1}$
\end_inset

 Score 来描述算法的质量（见图）。
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 11.4.Trading off precision and recall4.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:F-score"

\end_inset

F score
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
那么很明显，在使用查准率和查全率进行模型选择时，应该基于交叉验证集进行测试并且选择F值最大的模型，如课后题（图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:使用Precision&Recall进行模型选择"

\end_inset

）所示。
\end_layout

\begin_layout Standard
\align center
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 11.4.Trading off precision and recall5.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:使用Precision&Recall进行模型选择"

\end_inset

使用Precision&Recall进行模型选择
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Data for Machine Learning
\end_layout

\begin_layout Standard
上节课讨论的是误差矩阵，这节课将介绍机器学习系统设计中的另一重要方面，是关于训练模型时应该使用多大的训练集的。
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 11.5.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:海量数据原理(1)"

\end_inset

海量数据原理(1)
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename 11.5.2.png
	scale 75

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:海量数据原理(2)"

\end_inset

海量数据原理(2)
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_body
\end_document
